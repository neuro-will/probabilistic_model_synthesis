{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greek-republic",
   "metadata": {},
   "source": [
    "Notebook for initial development and testing for synthesizing FA models.\n",
    "\n",
    "The user specifies a number of individuals we observe data from.  For each of these individuals, we create a random number of observed variables (e.g., neurons) and associate with each of these variables some random properties (e.g., position or genetic information) generated uniformly from the unit square. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import torch\n",
    "\n",
    "from janelia_core.math.basic_functions import optimal_orthonormal_transform\n",
    "from janelia_core.ml.torch_distributions import CondGaussianDistribution\n",
    "from janelia_core.ml.torch_distributions import CondGammaDistribution\n",
    "from janelia_core.ml.extra_torch_modules import ConstantBoundedFcn\n",
    "from janelia_core.ml.extra_torch_modules import Tanh\n",
    "from janelia_core.visualization.image_generation import generate_2d_fcn_image\n",
    "from janelia_core.visualization.matrix_visualization import cmp_n_mats\n",
    "from janelia_core.ml.utils import list_torch_devices\n",
    "from janelia_core.ml.utils import torch_mod_to_fcn\n",
    "\n",
    "from probabilistic_model_synthesis.fa import FAMdl\n",
    "from probabilistic_model_synthesis.fa import Fitter\n",
    "from probabilistic_model_synthesis.fa import generate_basic_posteriors\n",
    "from probabilistic_model_synthesis.fa import generate_hypercube_prior_collection\n",
    "from probabilistic_model_synthesis.fa import generate_simple_prior_collection\n",
    "from probabilistic_model_synthesis.fa import VICollection\n",
    "from probabilistic_model_synthesis.math import MeanFcnTransformer\n",
    "from probabilistic_model_synthesis.math import StdFcnTransformer\n",
    "from probabilistic_model_synthesis.visualization import plot_torch_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-elite",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of individuals we simulate observing data from \n",
    "n_individuals = 5\n",
    "\n",
    "# Range of the number of variables we observe from each individual - the actual number of variables we observe from an\n",
    "# individual will be pulled uniformly from this range (inclusive)\n",
    "n_var_range = [10000, 12000]\n",
    "\n",
    "# Range of the number of samples we observe from each individual - the actual number we observe from each individual\n",
    "# will be unformly from this range (inclusive)\n",
    "n_smps_range = [1000, 1500]\n",
    "\n",
    "# Number of latent variables in the model\n",
    "n_latent_vars = 3\n",
    "\n",
    "# True if we should use GPUs for fitting if they are available\n",
    "use_gpus = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-fault",
   "metadata": {},
   "source": [
    "## Create the true prior distributions that relate parameters in the model to variable (e.g., neuron) properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_priors = generate_simple_prior_collection(n_prop_vars=2, n_latent_vars=n_latent_vars, \n",
    "                                               lm_mn_w_init_std=1.0, lm_std_w_init_std=.1,\n",
    "                                               mn_mn_w_init_std=1.0, mn_std_w_init_std=1.0,\n",
    "                                               psi_conc_f_w_init_std=2.0, psi_rate_f_w_init_std=1.0, \n",
    "                                               psi_conc_bias_mn=10.0, psi_rate_bias_mn=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-latter",
   "metadata": {},
   "source": [
    "## Generate properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_n_vars = np.random.randint(n_var_range[0], n_var_range[1]+1, n_individuals)\n",
    "ind_n_smps = np.random.randint(n_smps_range[0], n_smps_range[1]+1, n_individuals)\n",
    "ind_props = [torch.rand(size=[n_vars,2]) for n_vars in ind_n_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-absence",
   "metadata": {},
   "source": [
    "## Generate true FA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ind_true_fa_mdls = [FAMdl(lm=true_priors.lm_prior.sample(props), mn=true_priors.mn_prior.sample(props).squeeze(), \n",
    "                           psi=(true_priors.psi_prior.sample(props).squeeze()))\n",
    "                        for props in ind_props]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-energy",
   "metadata": {},
   "source": [
    "## Generate data from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ind_data = [mdl.sample(n_smps) for n_smps, mdl in zip(ind_n_smps, ind_true_fa_mdls)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-poland",
   "metadata": {},
   "source": [
    "## Fit new models together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_priors = generate_hypercube_prior_collection(n_latent_vars=n_latent_vars, \n",
    "                                                 hc_params={'n_divisions_per_dim': [40, 40], \n",
    "                                                            'dim_ranges': np.asarray([[0, 1.1], [0, 1.1]]), \n",
    "                                                            'n_div_per_hc_side_per_dim': [2, 2]})\n",
    "\n",
    "fit_posteriors = generate_basic_posteriors(n_obs_vars=ind_n_vars, n_smps=ind_n_smps, n_latent_vars=n_latent_vars)\n",
    "\n",
    "fit_mdls = [FAMdl(lm=None, mn=None, psi=None) for i in range(n_individuals)]\n",
    "\n",
    "vi_collections = [VICollection(data=data_i[1], props=props_i, mdl=mdl_i, posteriors=posteriors_i) \n",
    "                  for data_i, props_i,mdl_i, posteriors_i in zip(ind_data, ind_props, fit_mdls, fit_posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-sheep",
   "metadata": {},
   "source": [
    "## Set initial values of posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpus:\n",
    "    devices, _ = list_torch_devices()\n",
    "else:\n",
    "    devices = [torch.device('cpu')]\n",
    "    \n",
    "fitter = Fitter(vi_collections=vi_collections, priors=fit_priors, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.distribute(distribute_data=True, devices=devices)\n",
    "logs = [fitter.fit(500, milestones=[100, 300, 500, 700], update_int=100, init_lr=.1, skip_lm_kl=False, \n",
    "                 skip_mn_kl=False, skip_psi_kl=False) for fit_r in range(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dd64de",
   "metadata": {},
   "source": [
    "## Move the VI collections and priors back to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.distribute(devices=[torch.device('cpu')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-permission",
   "metadata": {},
   "source": [
    "## Examine lots of fitting performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-banks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitter.plot_log(logs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-scanning",
   "metadata": {},
   "source": [
    "## Look at model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_mdl = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_lm = vi_collections[exam_mdl].posteriors.lm_post(ind_props[exam_mdl]).detach().squeeze()\n",
    "fit_mn = vi_collections[exam_mdl].posteriors.mn_post(ind_props[exam_mdl]).detach().squeeze()\n",
    "fit_psi = vi_collections[exam_mdl].posteriors.psi_post.mode(ind_props[exam_mdl]).detach().squeeze()\n",
    "#fit_psi = vi_collections[exam_mdl].mdl.psi\n",
    "\n",
    "cmp_mdl = FAMdl(lm=fit_lm, mn=fit_mn, psi=fit_psi)\n",
    "true_mdl = ind_true_fa_mdls[exam_mdl]\n",
    "\n",
    "plt.figure()\n",
    "true_mdl.compare_models(true_mdl, cmp_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-sewing",
   "metadata": {},
   "source": [
    "### Visualize paraemters of the true prior distributions over the loading matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(n_latent_vars):\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plot_torch_dist(mn_f=true_priors.lm_prior.mn_f, std_f=true_priors.lm_prior.std_f, vis_dim=d, \n",
    "                    extra_title_str = ', d=' + str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-september",
   "metadata": {},
   "source": [
    "### Visualize paraemters of the fit prior distributions over the loading matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_vls = torch.rand(1000,2)\n",
    "o = optimal_orthonormal_transform(true_priors.lm_prior(rnd_vls).detach().numpy(), \n",
    "                                  fit_priors.lm_prior(rnd_vls).detach().numpy())\n",
    "\n",
    "\n",
    "class std_calculator(torch.nn.Module):\n",
    "    # A wrapper module for calculating standard deviation of conditional matrix product distributions\n",
    "    \n",
    "    def __init__(self, m):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate standard deviation\n",
    "        return torch.cat([d.std_f(x) for d in self.m.dists], dim=1)\n",
    "        \n",
    "std_calc = std_calculator(fit_priors.lm_prior)\n",
    "    \n",
    "fit_lm_mn_fcn = MeanFcnTransformer(o=o.transpose(), f=fit_priors.lm_prior.forward)\n",
    "fit_lm_std_fcn = StdFcnTransformer(o=o.transpose(), f=std_calc.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(n_latent_vars):\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plot_torch_dist(mn_f=fit_lm_mn_fcn, std_f=fit_lm_std_fcn, vis_dim=d, \n",
    "                    extra_title_str = ', d=' + str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-stuff",
   "metadata": {},
   "source": [
    "### Visualize parameters of the true prior distribution over the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "plot_torch_dist(mn_f=true_priors.mn_prior.mn_f, std_f=true_priors.mn_prior.std_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-opera",
   "metadata": {},
   "source": [
    "### Visualize parameters of the fit prior distribution over the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "plot_torch_dist(mn_f=fit_priors.mn_prior.mn_f, std_f=fit_priors.mn_prior.std_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-supervision",
   "metadata": {},
   "source": [
    "### Visualize parameters of the true prior distribution over private variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "plot_torch_dist(mn_f=true_priors.psi_prior.forward, std_f=true_priors.psi_prior.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-costs",
   "metadata": {},
   "source": [
    "### Visualize parameters of the fit prior distribution over private variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "plot_torch_dist(mn_f=fit_priors.psi_prior.forward, std_f=fit_priors.psi_prior.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b145721",
   "metadata": {},
   "source": [
    "## Visualize latent estimates for an example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_s = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93970dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn transformation to put estimated latents into same space as true latents\n",
    "with torch.no_grad():\n",
    "    true_lm = ind_true_fa_mdls[ex_s].lm.numpy()\n",
    "    est_lm = fit_posteriors[ex_s].lm_post(ind_props[ex_s]).numpy()\n",
    "    o = optimal_orthonormal_transform(true_lm, est_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get estimated latents in the right space\n",
    "est_latents = np.matmul(fit_posteriors[ex_s].latent_post.mns.detach().numpy(), o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d625f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latents\n",
    "true_latents = ind_data[ex_s][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for l_i in range(n_latent_vars):\n",
    "    ax = plt.subplot(n_latent_vars, 1, l_i+1)\n",
    "    plt.plot(true_latents[:, l_i], est_latents[:, l_i], 'r.')\n",
    "    ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcabb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ac59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
