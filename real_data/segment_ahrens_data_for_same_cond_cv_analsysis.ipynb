{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segments data for for fitting models to the same behavior across fish.  Here we break data up into a given number of disjoint sets that can later be combined to form train, validation and test sets.  \n",
    "\n",
    "We have in mind applications where we do cross-validation, but cross validation is performed in a non-standard way.  In particular, \n",
    "\n",
    "1) We break the data up into T disjoint folds for testing. \n",
    "\n",
    "2) However, unlike standard cross validation, all of the data which is not in the testing data for a fold, may or may not be used for training and validation.  The reason for this is that we want to look at model performance as the amount of data used for training and validation changes.  For this reason, only some of the data not used for testing may be used for training and validation. \n",
    "\n",
    "To facilitate (1) and (2) above, the idea is the user will specifiy a number of disjoint \"sets\" (e.g., 40) to break the data for a subject into.  Each set will be roughly balanced in the different types of behaviors that are present as well as swim vigor.  At model fitting time, the user can then form the sets into largers test sets for cross validation, and use a subset of the remaining sets for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ahrens_wbo.annotations import label_periods\n",
    "from ahrens_wbo.data_processing import SegmentTable\n",
    "from ahrens_wbo.data_processing import segment_dataset\n",
    "from ahrens_wbo.raw_data_processing import load_processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "ps['data_dir'] = r'/groups/bishop/bishoplab/projects/ahrens_wbo/data'\n",
    "\n",
    "# Specify subjects\n",
    "ps['subjects'] =  [1, 2, 5, 6, 8, 9, 10, 11]\n",
    "\n",
    "# Specify the number of sets we form\n",
    "ps['n_sets'] = 42\n",
    "\n",
    "# Specify size of chunks data up into - we form sets out of chunks\n",
    "ps['chunk_size'] = 5\n",
    "\n",
    "# Specify which behavioral channels we will use for calculating values to associate with each sample point\n",
    "ps['value_chs'] = [3, 4]\n",
    "\n",
    "# Specify how we will group the data\n",
    "ps['groups'] = OrderedDict([('phototaxis_ns', [{'period': 'phototaxis', 'shock': False}])])\n",
    "                                                                                                          \n",
    "# Specify value function\n",
    "ps['value_fnc'] = 'max'\n",
    "ps['random_vl_assignment'] = True\n",
    "    \n",
    "# Specify where we should save the segment information\n",
    "ps['save_folder'] = r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/'\n",
    "ps['save_name'] = r'phototaxis_ns_subjects_1_2_5_6_8_9_10_11_v2.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify segment ratios\n",
    "ps['segment_labels'] = ['set_' + str(i) for i in range(ps['n_sets'])]\n",
    "ps['segment_ratios'] = [1]*ps['n_sets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of labels for each dataset and swim values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading subject 1\n",
      "Done loading subject 2\n",
      "Done loading subject 5\n",
      "Done loading subject 6\n",
      "Done loading subject 8\n",
      "Done loading subject 9\n",
      "Done loading subject 10\n",
      "Done loading subject 11\n"
     ]
    }
   ],
   "source": [
    "n_subjects = len(ps['subjects'])\n",
    "labels = dict()\n",
    "values = dict()\n",
    "for subject_id in ps['subjects']:\n",
    "    dataset = load_processed_data(Path(ps['data_dir']) / ('subject_' + str(subject_id)), subject_id)\n",
    "    labels[subject_id] = label_periods(dataset.ts_data['stim']['vls'][:])\n",
    "    values[subject_id] = dataset.ts_data['behavior']['vls'][:, ps['value_chs']]\n",
    "    print('Done loading subject ' + str(subject_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['value_fnc'] == 'mean':\n",
    "    value_fnc = lambda x: np.mean(x)\n",
    "elif ps['value_fnc'] == 'max':\n",
    "    value_fnc = lambda x: np.max(x)\n",
    "else:\n",
    "    raise(ValueError('value_fcn is not recogonized'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_tables = OrderedDict()\n",
    "for s_n in ps['subjects']:\n",
    "    subj_values = np.mean(values[s_n], axis=1)\n",
    "    segment_tables[s_n] = segment_dataset(period_lbls=labels[s_n], groups=ps['groups'], \n",
    "                                               chunk_size=ps['chunk_size'],\n",
    "                                               segment_labels=ps['segment_labels'], \n",
    "                                               segment_ratios=ps['segment_ratios'], \n",
    "                                               vls=subj_values, vl_fnc=value_fnc, \n",
    "                                               random_vl_assignment=ps['random_vl_assignment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the segment tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_n in ps['subjects']:\n",
    "    segment_tables[s_n] = segment_tables[s_n].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(Path(ps['save_folder']) / ps['save_name'])\n",
    "rs = {'ps': ps, 'segment_tables': segment_tables}\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment tables saved to: /groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/phototaxis_ns_subjects_1_2_5_6_8_9_10_11_v2.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Segment tables saved to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e658792936ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegment_tables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)"
     ]
    }
   ],
   "source": [
    "segment_tables[1][0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
