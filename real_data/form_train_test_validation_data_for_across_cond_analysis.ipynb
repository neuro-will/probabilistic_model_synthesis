{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8364ea",
   "metadata": {},
   "source": [
    "This notebook is designed to be used in conjunction with the notebook \"segment_periods_for_transfer_analysis.\" That notebook breaks data for each subject into different sets, which can then be combined to form training, validation and testing data in this notebook.\n",
    "\n",
    "The idea behind the way we break up data is we want to do multiple analyses, each of the following form: \n",
    "\n",
    "    1) We identify a \"target\" fish - this is a fish we want to transfer model structure we learn from other fish to.  We also identify a \"target condition\" we observe this fish under. \n",
    "    \n",
    "    2) We identify a number of \"transfer\" fish - these are fish we will observe in conditions different from those we observe the target fish in. We want to transfer what we learn about model structure under these conditions to the target fish. \n",
    "    \n",
    "    3) We will from our training and validation data in two ways.  In the first way, the training and validation data for each fish consists of a different condition.  In the second way, the training and validation data for all fish consists of the same condition as the condition for the target fish.  We make sure the total amount of training and validation data used in both cases in the same. \n",
    "    \n",
    "    4) We then test the performance of the models for the target fish on the conditions outside of its training data (as well as on the condition in its training data). What we hope to see is that model performance improves on the conditions outside of the target fish's training condition when we synthesize a model for the target fish when the training data for the other fish is of the other conditions relative to when all fish have the same condition.  This would show our framework is able to transfer model structure across fish even when those fish are observed in different conditions. \n",
    "    \n",
    "To this end, this script will generate and save two assignments of data to train, test and validation for each target fish and target fish condition.   We generate such an assignment for each type of condition we observe in the target fish (e.g., if we have OMR L, R and F data for our fish, we can do three seperate analyses where we assume we only observe one of these conditions in the target fish and then the other two in the transfer fish).  The two assignments correspond to when (1) we observe different conditions across fish and (2) when we observe the same condition in all fish. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d41b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "from ahrens_wbo.data_processing import SegmentTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88965809",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7738520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Specify where the segment table created by segment_ahrens_data_for_across_cond_analsysis.ipynb is saved\n",
    "ps['segment_table_folder'] = r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/'\n",
    "ps['segment_table_file'] = r'omr_l_r_f_ns_across_cond_segments_8_9_10_11.pkl'\n",
    "\n",
    "# Specify the different conditions we want to test - there should be three of these\n",
    "ps['test_groups'] = ['omr_l_ns', 'omr_r_ns', 'omr_f_ns']\n",
    "\n",
    "# Specify the different subjects we use in the analysis - there should be three of these\n",
    "ps['subjects'] = [8, 9, 11]\n",
    "\n",
    "# Specify the percentage of data for each target subject we use for training.  (Note because we \n",
    "# balance data across fish, we may not use this much, so this is the max we can use.)\n",
    "ps['tgt_subj_train_percentage'] = .7\n",
    "\n",
    "# Specify the percentage of data for each target subject train condition we use for validation. (Note because we \n",
    "# balance data across fish, we may not use this much, so this is the max we can use.)\n",
    "ps['tgt_subj_validation_percentage'] = .15\n",
    "\n",
    "# Specify the folder we should save the fold structures in\n",
    "ps['save_folder'] = r'' #r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/'\n",
    "\n",
    "# Specify a base string to prepend to file names with the fold structures\n",
    "ps['save_str'] = 'ac_an'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d178470",
   "metadata": {},
   "source": [
    "## Load the segment tables and get basic information we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2c006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_table_path = Path(ps['segment_table_folder']) / ps['segment_table_file']\n",
    "with open(segment_table_path, 'rb') as f:\n",
    "    seg_table_data = pickle.load(f)\n",
    "    \n",
    "seg_tables = {s_n: SegmentTable.from_dict(seg_table) for s_n, seg_table in seg_table_data['segment_tables'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36fa92",
   "metadata": {},
   "source": [
    "## Define helper functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7351be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_assign_chunks(n_max_chunks, n_train_chunks, n_validation_chunks):\n",
    "    chunk_order = random.permutation(n_max_chunks)\n",
    "    train_chunks = chunk_order[:n_train_chunks]\n",
    "    validation_chunks = chunk_order[n_train_chunks:n_train_chunks+n_validation_chunks]\n",
    "    test_chunks = chunk_order[n_train_chunks+n_validation_chunks:]\n",
    "    \n",
    "    return train_chunks, validation_chunks, test_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1d75ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_subj = 8\n",
    "tgt_subj_train_group = 'omr_l_ns'\n",
    "train_percentage = .7\n",
    "validation_percentage = .15\n",
    "trans_subjs = [9, 11]\n",
    "trans_train_groups = ['omr_r_ns', 'omr_f_ns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4dfe6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_transfer_subjs = len(trans_subjs)\n",
    "\n",
    "all_subjs = [tgt_subj] + trans_subjs\n",
    "all_across_cond_train_conds = [tgt_subj_train_group] + trans_train_groups\n",
    "\n",
    "n_max_across_cond_train_chunks = [np.floor(train_percentage*seg_tables[s_n].n_group_segments(grp)) \n",
    "                                  for s_n, grp in zip(all_subjs, all_across_cond_train_conds)]\n",
    "\n",
    "n_max_across_cond_validation_chunks = [np.floor(validation_percentage*seg_tables[s_n].n_group_segments(grp)) \n",
    "                                  for s_n, grp in zip(all_subjs, all_across_cond_train_conds)]\n",
    "\n",
    "n_max_within_cond_train_chunks = [np.floor(train_percentage*seg_tables[s_n].n_group_segments(tgt_subj_train_group)) \n",
    "                                  for s_n in all_subjs]\n",
    "\n",
    "n_max_within_cond_validation_chunks = [np.floor(validation_percentage*seg_tables[s_n].n_group_segments(tgt_subj_train_group)) \n",
    "                                  for s_n in all_subjs]\n",
    "\n",
    "n_train_chunks = int(np.min(np.stack([n_max_across_cond_train_chunks, n_max_within_cond_train_chunks])))\n",
    "n_validation_chunks = int(np.min(np.stack([n_max_across_cond_validation_chunks, n_max_within_cond_validation_chunks])))\n",
    "\n",
    "# Form across condition assignments\n",
    "assignments = dict()\n",
    "for s_n, grp in zip(all_subjs, all_across_cond_train_conds):\n",
    "    num_segs_n = seg_tables[s_n].n_group_segments(grp)\n",
    "    \n",
    "    train_chunks_n, validation_chunks_n, test_chunks_n = randomly_assign_chunks(n_max_chunks=num_segs_n, \n",
    "                                                                                n_train_chunks=n_train_chunks,\n",
    "                                                                                n_validation_chunks=n_validation_chunks)\n",
    "    \n",
    "    assignments[s_n] = {'train': {grp: ['set_' + str(i) for i in train_chunks_n]},\n",
    "                     'validation': {grp: ['set_' + str(i) for i in validation_chunks_n]},\n",
    "                     'test': {grp: ['set_' + str(i) for i in test_chunks_n] }}\n",
    "    \n",
    "    # TODO: Test data needs to include other conditions as well\n",
    "\n",
    "\n",
    "\n",
    "# Form within condition assignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b72bf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'train': {'omr_l_ns': ['set_6',\n",
       "    'set_9',\n",
       "    'set_80',\n",
       "    'set_35',\n",
       "    'set_23',\n",
       "    'set_71',\n",
       "    'set_43',\n",
       "    'set_85',\n",
       "    'set_34',\n",
       "    'set_102',\n",
       "    'set_20',\n",
       "    'set_45',\n",
       "    'set_97',\n",
       "    'set_37',\n",
       "    'set_31',\n",
       "    'set_1',\n",
       "    'set_32',\n",
       "    'set_63',\n",
       "    'set_91',\n",
       "    'set_25',\n",
       "    'set_98']},\n",
       "  'validation': {'omr_l_ns': ['set_4', 'set_60', 'set_87', 'set_92']},\n",
       "  'test': {'omr_l_ns': ['set_0',\n",
       "    'set_100',\n",
       "    'set_11',\n",
       "    'set_49',\n",
       "    'set_82',\n",
       "    'set_83',\n",
       "    'set_99',\n",
       "    'set_107',\n",
       "    'set_22',\n",
       "    'set_13',\n",
       "    'set_21',\n",
       "    'set_77',\n",
       "    'set_33',\n",
       "    'set_104',\n",
       "    'set_28',\n",
       "    'set_106',\n",
       "    'set_66',\n",
       "    'set_12',\n",
       "    'set_74',\n",
       "    'set_94',\n",
       "    'set_86',\n",
       "    'set_24',\n",
       "    'set_89',\n",
       "    'set_105',\n",
       "    'set_2',\n",
       "    'set_40',\n",
       "    'set_3',\n",
       "    'set_70',\n",
       "    'set_8',\n",
       "    'set_73',\n",
       "    'set_52',\n",
       "    'set_57',\n",
       "    'set_75',\n",
       "    'set_78',\n",
       "    'set_46',\n",
       "    'set_42',\n",
       "    'set_27',\n",
       "    'set_26',\n",
       "    'set_103',\n",
       "    'set_48',\n",
       "    'set_54',\n",
       "    'set_64',\n",
       "    'set_81',\n",
       "    'set_69',\n",
       "    'set_96',\n",
       "    'set_101',\n",
       "    'set_51',\n",
       "    'set_84',\n",
       "    'set_7',\n",
       "    'set_88',\n",
       "    'set_14',\n",
       "    'set_19',\n",
       "    'set_68',\n",
       "    'set_53',\n",
       "    'set_95',\n",
       "    'set_67',\n",
       "    'set_29',\n",
       "    'set_5',\n",
       "    'set_58',\n",
       "    'set_16',\n",
       "    'set_18',\n",
       "    'set_59',\n",
       "    'set_61',\n",
       "    'set_62',\n",
       "    'set_39',\n",
       "    'set_41',\n",
       "    'set_17',\n",
       "    'set_55',\n",
       "    'set_76',\n",
       "    'set_65',\n",
       "    'set_47',\n",
       "    'set_79',\n",
       "    'set_44',\n",
       "    'set_30',\n",
       "    'set_15',\n",
       "    'set_72',\n",
       "    'set_90',\n",
       "    'set_36',\n",
       "    'set_38',\n",
       "    'set_10',\n",
       "    'set_50',\n",
       "    'set_56',\n",
       "    'set_93']}},\n",
       " 9: {'train': {'omr_r_ns': ['set_3',\n",
       "    'set_25',\n",
       "    'set_6',\n",
       "    'set_13',\n",
       "    'set_31',\n",
       "    'set_10',\n",
       "    'set_2',\n",
       "    'set_30',\n",
       "    'set_12',\n",
       "    'set_19',\n",
       "    'set_14',\n",
       "    'set_23',\n",
       "    'set_8',\n",
       "    'set_9',\n",
       "    'set_15',\n",
       "    'set_11',\n",
       "    'set_26',\n",
       "    'set_22',\n",
       "    'set_7',\n",
       "    'set_18',\n",
       "    'set_16']},\n",
       "  'validation': {'omr_r_ns': ['set_20', 'set_4', 'set_27', 'set_17']},\n",
       "  'test': {'omr_r_ns': ['set_1',\n",
       "    'set_28',\n",
       "    'set_24',\n",
       "    'set_5',\n",
       "    'set_21',\n",
       "    'set_0',\n",
       "    'set_29']}},\n",
       " 11: {'train': {'omr_f_ns': ['set_16',\n",
       "    'set_10',\n",
       "    'set_6',\n",
       "    'set_4',\n",
       "    'set_13',\n",
       "    'set_22',\n",
       "    'set_12',\n",
       "    'set_18',\n",
       "    'set_21',\n",
       "    'set_23',\n",
       "    'set_19',\n",
       "    'set_29',\n",
       "    'set_0',\n",
       "    'set_24',\n",
       "    'set_5',\n",
       "    'set_1',\n",
       "    'set_8',\n",
       "    'set_11',\n",
       "    'set_27',\n",
       "    'set_3',\n",
       "    'set_30']},\n",
       "  'validation': {'omr_f_ns': ['set_26', 'set_25', 'set_20', 'set_17']},\n",
       "  'test': {'omr_f_ns': ['set_7',\n",
       "    'set_2',\n",
       "    'set_14',\n",
       "    'set_9',\n",
       "    'set_15',\n",
       "    'set_31',\n",
       "    'set_28']}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "496b4f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_validation_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398b04fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_subj_group_fold(seg_tables, tgt_subj, tgt_subj_train_group, tgt_subj_train_percentage,  \n",
    "                         tgt_subj_validation_percentage, trans_subjs, trans_train_groups):\n",
    "    \"\"\" Assigns training, validation and testing data for a transfer analysis for one target fish. \n",
    "    \n",
    "    Here, the user specifies a \"target\" fish (a fish we want to transfer model structure to) and the condition\n",
    "    that we get to see in the training data for that fish (e.g., OMR Left).  The user also specifies \"transfer\"\n",
    "    fish and the conditions we get to see in each of those fish (e.g., OMR Right, OMR Forward).  This function \n",
    "    then assigns training data so that we see a different condition in each fish. In particular, we see the condition \n",
    "    for the training fish and each of the specified conditions in each of the transfer fish. \n",
    "           \n",
    "    When assigning the training data, this function will ensure the amount of training data is the same for all\n",
    "    training conditions and fish. \n",
    "    \n",
    "    This function will also assign validation and test data for the target fish (but it will not \n",
    "    assign validation and test data for the transfer fish, since we expect to be doing early stopping and testing \n",
    "    based only on the target fish).  Test data consists of all the data in each of the train conditions for the\n",
    "    transfer fish and a percentage (given by 1 - tgt_subj_train_percentage - tgt_subj_validation_percentage) of the \n",
    "    data for the train condition for the target fish.  The validation data validation data is the same condition as \n",
    "    the train data, and the amount of data will be roughly equal to tgt_subj_validation_percentage (only roughly equal\n",
    "    due to the discrete amount of data that is available) of the amount of data available for the condition.  We try \n",
    "    to roughly ensure balance in the swimming strength in training, testing and validation for the target condition \n",
    "    data by randomly assigning the top sets in the segment table to the train, test and validation \n",
    "    data (to see how we sort sets in the segment tables by swimming strength, see the notes below).\n",
    "    \n",
    "    A couple of important final notes: \n",
    "    \n",
    "    1) The segment table input is expected to be sorted by swimming strength (the notebook, \n",
    "    segment_ahrens_data_for_across_cond_analysis does this).  In many cases, we may not be able to use all the \n",
    "    data of a condition for training, testing and validation (see below).  In that case, we use the data with the \n",
    "    strongest swimming signals (specifically, we use the top sets in the segment table, when sets are sorted by\n",
    "    swimming strength).\n",
    "    \n",
    "    2) Due to the need to balance the amount of training data across fish, we may not use all the available \n",
    "    training data for a condition in a fish.  \n",
    "    \n",
    "    3) In some cases, there may be much more data for the target condition and target fish than there is for the \n",
    "    transfer conditions and transfer fish. In this case, we can't use the full tgt_subj_train_percentage percent\n",
    "    of data for the training data, and we assign the unused data to testing. \n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    n_transfer_subjs = len(trans_subjs)\n",
    "    \n",
    "    # See how many segments are available for training and validation in each subject\n",
    "    \n",
    "    n_tgt_subj_segs = seg_tables[tgt_subj].n_group_segments(tgt_subj_train_group)\n",
    "    n_tgt_subj_train_segs = int(np.floor(n_tgt_subj_segs*tgt_subj_train_percentage))\n",
    "    n_tgt_subj_validation_segs = int(np.floor(n_tgt_subj_segs*tgt_subj_validation_percentage))\n",
    "\n",
    "    n_trans_subj_train_segs = [seg_tables[s_n].n_group_segments(grp) \n",
    "                               for s_n, grp in zip(trans_subjs, trans_train_groups)]\n",
    "    \n",
    "    # Determine the number of segments used for training - this is the min number available across all subjects\n",
    "    n_train_segs = np.min(n_trans_subj_train_segs + [n_tgt_subj_train_segs])\n",
    "    \n",
    "    # Determine number of segments for testing the train condition in the target fish\n",
    "    n_tgt_subj_train_cond_test_segs = n_tgt_subj_segs - n_train_segs - n_tgt_subj_validation_segs\n",
    "    \n",
    "    # Form our fold structure for the target fish here\n",
    "    #n_tgt_fish_segs = n_train_segs + n_tgt_subj_validation_segs\n",
    "    tgt_seg_nums = random.permutation(n_tgt_subj_segs)\n",
    "    tgt_train_seg_nums = tgt_seg_nums[0:n_train_segs]\n",
    "    tgt_validation_seg_nums = tgt_seg_nums[n_train_segs:n_train_segs+n_tgt_subj_validation_segs]\n",
    "    tgt_train_cond_test_seg_nums = tgt_seg_nums[n_train_segs+n_tgt_subj_validation_segs:]\n",
    "      \n",
    "    tgt_fish_fold = {'train': {tgt_subj_train_group: ['set_' + str(n) for n in tgt_train_seg_nums]},\n",
    "                     'validation': {tgt_subj_train_group: ['set_' + str(n) for n in tgt_validation_seg_nums]},\n",
    "                     'test': {tgt_subj_train_group: ['set_' + str(n) for n in tgt_train_cond_test_seg_nums] }}\n",
    "    \n",
    "    for s_i in range(n_transfer_subjs):\n",
    "        tgt_fish_fold['test'][trans_train_groups[s_i]] = ['set_' + str(n) \n",
    "                                    for n in range(seg_tables[tgt_subj].n_group_segments(trans_train_groups[s_i]))]\n",
    "                     \n",
    "    # Form our fold structure for the transfer fish\n",
    "    transfer_fish_folds = [{'train': {trans_train_groups[s_i]: ['set_' + str(n) for n in range(n_train_segs)]},\n",
    "                            'validation': None, \n",
    "                            'test': None}\n",
    "                           for s_i, s_n in enumerate(trans_subjs)]\n",
    "\n",
    "    return {tgt_subj: tgt_fish_fold, \n",
    "            trans_subjs[0]: transfer_fish_folds[0], \n",
    "            trans_subjs[1]: transfer_fish_folds[1]}\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeed26",
   "metadata": {},
   "source": [
    "## Form fold structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86644051",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cond_fold_strs = dict()\n",
    "single_cond_fold_strs = dict()\n",
    "for tgt_subj in ps['subjects']:\n",
    "    fish_multi_cond_folds = dict()\n",
    "    fish_single_cond_folds = dict()\n",
    "    for tgt_cond in ps['test_groups']:\n",
    "        trans_subjs = [s_n for s_n in ps['subjects'] if s_n != tgt_subj]\n",
    "        trans_conds = [cond for cond in ps['test_groups']  if cond != tgt_cond]\n",
    "        \n",
    "        multi_cond_folds = form_subj_group_fold(seg_tables=seg_tables, \n",
    "                                                tgt_subj=tgt_subj, \n",
    "                                                tgt_subj_train_group=tgt_cond, \n",
    "                                                tgt_subj_train_percentage=ps['tgt_subj_train_percentage'], \n",
    "                                                tgt_subj_validation_percentage=ps['tgt_subj_validation_percentage'],\n",
    "                                                trans_subjs=trans_subjs, \n",
    "                                                trans_train_groups=trans_conds)\n",
    "        \n",
    "        transfer_subjs = set(ps['subjects']) - set([tgt_subj])\n",
    "        single_cond_folds = copy.deepcopy(multi_cond_folds)\n",
    "        for transfer_subj in transfer_subjs:\n",
    "            n_train_segs = len(single_cond_folds[tgt_subj]['train'][tgt_cond])\n",
    "            single_cond_folds[transfer_subj]['train'] = {tgt_cond: ['set_' + str(i) for i in range(n_train_segs)]}\n",
    "\n",
    "        fish_multi_cond_folds[tgt_cond] = multi_cond_folds\n",
    "        fish_single_cond_folds[tgt_cond] = single_cond_folds\n",
    "    \n",
    "    multi_cond_fold_strs[tgt_subj] = fish_multi_cond_folds\n",
    "    single_cond_fold_strs[tgt_subj] = fish_single_cond_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d535e8",
   "metadata": {},
   "source": [
    "Rearrange the fold structures so they are organized so that the fish is the first level and then fold is the second level - this will then allow the fold structures to be used seamlessly with our standard fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ece7b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_multi_cond_fold_strs = dict()\n",
    "new_single_cond_fold_strs = dict()\n",
    "for tgt_subj in ps['subjects']:\n",
    "    new_multi_cond_fold_strs[tgt_subj] = dict()\n",
    "    new_single_cond_fold_strs[tgt_subj] = dict()\n",
    "    for s_n in ps['subjects']:\n",
    "        new_multi_cond_fold_strs[tgt_subj][s_n] = dict()\n",
    "        new_single_cond_fold_strs[tgt_subj][s_n] = dict()\n",
    "        for tgt_cond in ps['test_groups']:\n",
    "            new_multi_cond_fold_strs[tgt_subj][s_n][tgt_cond] = multi_cond_fold_strs[tgt_subj][tgt_cond][s_n]\n",
    "            new_single_cond_fold_strs[tgt_subj][s_n][tgt_cond] = single_cond_fold_strs[tgt_subj][tgt_cond][s_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf28b1",
   "metadata": {},
   "source": [
    "## Save fold structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_subj in ps['subjects']:\n",
    "    multi_cond_folds = new_multi_cond_fold_strs[tgt_subj]\n",
    "    single_cond_folds = new_single_cond_fold_strs[tgt_subj]\n",
    "    \n",
    "    multi_cond_file_name = ps['save_str'] + '_tgt_' + str(tgt_subj) + '_multi_cond_folds.pkl'\n",
    "    single_cond_file_name = ps['save_str'] + '_tgt_' + str(tgt_subj) + '_single_cond_folds.pkl'\n",
    "    \n",
    "    multi_cond_path = Path(ps['save_folder']) / multi_cond_file_name\n",
    "    single_cond_path = Path(ps['save_folder']) / single_cond_file_name\n",
    "    \n",
    "    with open(multi_cond_path, 'wb') as f:\n",
    "        pickle.dump(multi_cond_folds, f)\n",
    "    with open(single_cond_path, 'wb') as f:\n",
    "        pickle.dump(single_cond_folds, f)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49a1969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'omr_l_ns': {'train': {'omr_r_ns': ['set_0',\n",
       "    'set_1',\n",
       "    'set_2',\n",
       "    'set_3',\n",
       "    'set_4',\n",
       "    'set_5',\n",
       "    'set_6',\n",
       "    'set_7',\n",
       "    'set_8',\n",
       "    'set_9',\n",
       "    'set_10',\n",
       "    'set_11',\n",
       "    'set_12',\n",
       "    'set_13',\n",
       "    'set_14',\n",
       "    'set_15',\n",
       "    'set_16',\n",
       "    'set_17',\n",
       "    'set_18',\n",
       "    'set_19',\n",
       "    'set_20',\n",
       "    'set_21',\n",
       "    'set_22',\n",
       "    'set_23',\n",
       "    'set_24',\n",
       "    'set_25',\n",
       "    'set_26',\n",
       "    'set_27',\n",
       "    'set_28',\n",
       "    'set_29',\n",
       "    'set_30',\n",
       "    'set_31']},\n",
       "  'validation': None,\n",
       "  'test': None},\n",
       " 'omr_r_ns': {'train': {'omr_l_ns': ['set_0',\n",
       "    'set_1',\n",
       "    'set_2',\n",
       "    'set_3',\n",
       "    'set_4',\n",
       "    'set_5',\n",
       "    'set_6',\n",
       "    'set_7',\n",
       "    'set_8',\n",
       "    'set_9',\n",
       "    'set_10',\n",
       "    'set_11',\n",
       "    'set_12',\n",
       "    'set_13',\n",
       "    'set_14',\n",
       "    'set_15',\n",
       "    'set_16',\n",
       "    'set_17',\n",
       "    'set_18',\n",
       "    'set_19',\n",
       "    'set_20',\n",
       "    'set_21',\n",
       "    'set_22',\n",
       "    'set_23',\n",
       "    'set_24',\n",
       "    'set_25',\n",
       "    'set_26',\n",
       "    'set_27',\n",
       "    'set_28',\n",
       "    'set_29',\n",
       "    'set_30',\n",
       "    'set_31']},\n",
       "  'validation': None,\n",
       "  'test': None},\n",
       " 'omr_f_ns': {'train': {'omr_l_ns': ['set_0',\n",
       "    'set_1',\n",
       "    'set_2',\n",
       "    'set_3',\n",
       "    'set_4',\n",
       "    'set_5',\n",
       "    'set_6',\n",
       "    'set_7',\n",
       "    'set_8',\n",
       "    'set_9',\n",
       "    'set_10',\n",
       "    'set_11',\n",
       "    'set_12',\n",
       "    'set_13',\n",
       "    'set_14',\n",
       "    'set_15',\n",
       "    'set_16',\n",
       "    'set_17',\n",
       "    'set_18',\n",
       "    'set_19',\n",
       "    'set_20',\n",
       "    'set_21',\n",
       "    'set_22',\n",
       "    'set_23',\n",
       "    'set_24',\n",
       "    'set_25',\n",
       "    'set_26',\n",
       "    'set_27',\n",
       "    'set_28',\n",
       "    'set_29',\n",
       "    'set_30',\n",
       "    'set_31']},\n",
       "  'validation': None,\n",
       "  'test': None}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_multi_cond_fold_strs[8][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688794c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
