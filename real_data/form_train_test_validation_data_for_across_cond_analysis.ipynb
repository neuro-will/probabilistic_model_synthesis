{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8364ea",
   "metadata": {},
   "source": [
    "This notebook is designed to be used in conjunction with the notebook \"segment_periods_for_transfer_analysis.\" That notebook breaks data for each subject into different sets, which can then be combined to form training, validation and testing data in this notebook.\n",
    "\n",
    "The idea behind the way we break up data is we want to do multiple analyses, each of the following form: \n",
    "\n",
    "    1) We identify a \"target\" fish - this is a fish we want to transfer model structure we learn from other fish to.  We also identify a \"target condition\" we observe this fish under. \n",
    "    \n",
    "    2) We identify a number of \"transfer\" fish - these are fish we will observe in conditions different from those we observe the target fish in. We want to transfer what we learn about model structure under these conditions to the target fish. \n",
    "    \n",
    "    3) We will from our training and validation data in two ways.  In the first way, the training and validation data for each fish consists of a different condition.  In the second way, the training and validation data for all fish consists of the same condition as the condition for the target fish.  We make sure the total amount of training and validation data used in both cases in the same. \n",
    "    \n",
    "    4) We then test the performance of the models for the target fish on the conditions outside of its training data (as well as on the condition in its training data). What we hope to see is that model performance improves on the conditions outside of the target fish's training condition when we synthesize a model for the target fish when the training data for the other fish is of the other conditions relative to when all fish have the same condition.  This would show our framework is able to transfer model structure across fish even when those fish are observed in different conditions. \n",
    "    \n",
    "To this end, this script will generate and save two assignments of data to train, test and validation for each target fish and target fish condition.   We generate such an assignment for each type of condition we observe in the target fish (e.g., if we have OMR L, R and F data for our fish, we can do three seperate analyses where we assume we only observe one of these conditions in the target fish and then the other two in the transfer fish).  The two assignments correspond to when (1) we observe different conditions across fish and (2) when we observe the same condition in all fish. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33d41b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465f6ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "from ahrens_wbo.data_processing import SegmentTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88965809",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7738520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Specify where the segment table created by segment_ahrens_data_for_across_cond_analsysis.ipynb is saved\n",
    "ps['segment_table_folder'] = r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/'\n",
    "ps['segment_table_file'] = r'omr_l_r_f_ns_across_cond_segments_8_9_10_11.pkl'\n",
    "\n",
    "# Specify the different conditions we want to test - there should be three of these\n",
    "ps['test_groups'] = ['omr_l_ns', 'omr_r_ns', 'omr_f_ns']\n",
    "\n",
    "# Specify the different subjects we use in the analysis - there should be three of these\n",
    "ps['subjects'] = [8, 9, 11]\n",
    "\n",
    "# Specify the percentage of data for each target subject we use for training.  (Note because we \n",
    "# balance data across fish, we may not use this much, so this is the max we can use.)\n",
    "ps['train_percentage'] = .7\n",
    "\n",
    "# Specify the percentage of data for each target subject train condition we use for validation. (Note because we \n",
    "# balance data across fish, we may not use this much, so this is the max we can use.)\n",
    "ps['validation_percentage'] = .15\n",
    "\n",
    "# Specify the folder we should save the fold structures in\n",
    "ps['save_folder'] = r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/'\n",
    "\n",
    "# Specify a base string to prepend to file names with the fold structures\n",
    "ps['save_str'] = 'ac_an'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d178470",
   "metadata": {},
   "source": [
    "## Load the segment tables and get basic information we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b2c006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_table_path = Path(ps['segment_table_folder']) / ps['segment_table_file']\n",
    "with open(segment_table_path, 'rb') as f:\n",
    "    seg_table_data = pickle.load(f)\n",
    "    \n",
    "seg_tables = {s_n: SegmentTable.from_dict(seg_table) for s_n, seg_table in seg_table_data['segment_tables'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36fa92",
   "metadata": {},
   "source": [
    "## Define helper functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7351be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_assign_chunks(n_max_chunks, n_train_chunks, n_validation_chunks):\n",
    "    chunk_order = random.permutation(n_max_chunks)\n",
    "    train_chunks = chunk_order[:n_train_chunks]\n",
    "    validation_chunks = chunk_order[n_train_chunks:n_train_chunks+n_validation_chunks]\n",
    "    test_chunks = chunk_order[n_train_chunks+n_validation_chunks:]\n",
    "    \n",
    "    return train_chunks, validation_chunks, test_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfe6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_for_one_tgt_subj(seg_tables, tgt_subj, tgt_subj_train_group, train_percentage, validation_percentage, trans_subjs, \n",
    "                            trans_train_groups):\n",
    "    \"\"\" Assigns train, validation & test data for a given target subject and target condition. \n",
    "    \n",
    "    This function will generate two assignments: one for across condition fitting and one for within condition fitting.\n",
    "    The across condition fitting is when the conditions in the training data for each subject are as listed in\n",
    "    the tgt_subj_train_group and trans_train_groups inputs, which can be different.  The within condition fitting\n",
    "    is when the training data for each subject is all of the tgt_subj_train_group condition. \n",
    "    \n",
    "    This function will ensure the total amount of training and validation data for each subject is the same in\n",
    "    both types of assignments.  It does this by finding the subject and condition with the smallest number of\n",
    "    chunks, and then:\n",
    "        1) Setting train_percentage of that min number of chunks as the number of training chunks \n",
    "           for all conditions and subjects\n",
    "        2) Setting validation_percentage of that min number of chunks as the number of validation chunks\n",
    "           for all conditions and subjects\n",
    "    \n",
    "    For each subject, the testing data will always be all chunks across all conditions not in its train and validation\n",
    "    data. \n",
    "    \n",
    "    To ensure a rough balance of swim vigor, we randomly pick the particular chunks assigned to train, validation and \n",
    "    test.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        seg_tables: The segment tables for all subjects\n",
    "        \n",
    "        tgt_subj: The target subject\n",
    "        \n",
    "        tgt_subj_train_group: The target condition\n",
    "        \n",
    "        train_percentage: The max percentage of training data we can use for any subject \n",
    "        \n",
    "        validation_percentage: The max percentage of validation data we can use for any subject\n",
    "        \n",
    "        trans_subjs: The base or transfer subjects\n",
    "        \n",
    "        trans_train_groups: The conditions in the training data for the transfer subjects\n",
    "        \n",
    "    Returns: \n",
    "    \n",
    "        across_assignments: The assignments for the across condition analyses.  A dictionary with keys \n",
    "        corresponding to subjects.  Each entry is itself a dictionary with the keys, 'train', 'validation' \n",
    "        and 'test'.  Each of these is another dictionary with keys corresponding to groups and values correspoding\n",
    "        to the sets (which are the same as chunks in this case) that should be pulled from that group. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n_transfer_subjs = len(trans_subjs)\n",
    "\n",
    "    all_subjs = [tgt_subj] + trans_subjs\n",
    "    all_across_cond_train_conds = [tgt_subj_train_group] + trans_train_groups\n",
    "\n",
    "    n_max_across_cond_train_chunks = [np.floor(train_percentage*seg_tables[s_n].n_group_segments(grp)) \n",
    "                                      for s_n, grp in zip(all_subjs, all_across_cond_train_conds)]\n",
    "\n",
    "    n_max_across_cond_validation_chunks = [np.floor(validation_percentage*seg_tables[s_n].n_group_segments(grp)) \n",
    "                                           for s_n, grp in zip(all_subjs, all_across_cond_train_conds)]\n",
    "\n",
    "    n_max_within_cond_train_chunks = [np.floor(train_percentage*seg_tables[s_n].n_group_segments(tgt_subj_train_group)) \n",
    "                                      for s_n in all_subjs]\n",
    "\n",
    "    n_max_within_cond_validation_chunks = [np.floor(validation_percentage*seg_tables[s_n].n_group_segments(tgt_subj_train_group)) \n",
    "                                           for s_n in all_subjs]\n",
    "\n",
    "    n_train_chunks = int(np.min(np.stack([n_max_across_cond_train_chunks, n_max_within_cond_train_chunks])))\n",
    "    n_validation_chunks = int(np.min(np.stack([n_max_across_cond_validation_chunks, \n",
    "                                               n_max_within_cond_validation_chunks])))\n",
    "\n",
    "    # Form across condition assignments\n",
    "    across_assignments = dict()\n",
    "    for s_n, grp in zip(all_subjs, all_across_cond_train_conds):\n",
    "        num_segs_n = seg_tables[s_n].n_group_segments(grp)\n",
    "    \n",
    "        train_chunks_n, validation_chunks_n, test_chunks_n = randomly_assign_chunks(n_max_chunks=num_segs_n, \n",
    "                                                                            n_train_chunks=n_train_chunks,\n",
    "                                                                            n_validation_chunks=n_validation_chunks)\n",
    "    \n",
    "        across_assignments[s_n] = {'train': {grp: ['set_' + str(i) for i in train_chunks_n]},\n",
    "                                   'validation': {grp: ['set_' + str(i) for i in validation_chunks_n]},\n",
    "                                   'test': {grp: ['set_' + str(i) for i in test_chunks_n] }}\n",
    "    \n",
    "        other_test_grps = list(set(all_across_cond_train_conds) - set([grp]))\n",
    "        for other_grp in other_test_grps:\n",
    "            other_grp_n_chunks = seg_tables[s_n].n_group_segments(other_grp)\n",
    "            across_assignments[s_n]['test'][other_grp] = ['set_' + str(i) for i in range(other_grp_n_chunks)]\n",
    "    \n",
    "    # Form within condition assignments\n",
    "    within_assignments = dict()\n",
    "    within_assignments[tgt_subj] = copy.deepcopy(across_assignments[tgt_subj])\n",
    "    for s_n in trans_subjs:\n",
    "        num_segs_n = seg_tables[s_n].n_group_segments(tgt_subj_train_group)\n",
    "        train_chunks_n, validation_chunks_n, test_chunks_n = randomly_assign_chunks(n_max_chunks=num_segs_n, \n",
    "                                                                                n_train_chunks=n_train_chunks,\n",
    "                                                                                n_validation_chunks=n_validation_chunks)\n",
    "    \n",
    "        within_assignments[s_n] = {'train': {tgt_subj_train_group: ['set_' + str(i) for i in train_chunks_n]},\n",
    "                                   'validation': {tgt_subj_train_group: ['set_' + str(i) for i in validation_chunks_n]},\n",
    "                                   'test': {tgt_subj_train_group: ['set_' + str(i) for i in test_chunks_n] }}\n",
    "    \n",
    "        for other_grp in trans_train_groups:\n",
    "            other_grp_n_chunks = seg_tables[s_n].n_group_segments(other_grp)\n",
    "            within_assignments[s_n]['test'][other_grp] = ['set_' + str(i) for i in range(other_grp_n_chunks)]\n",
    "            \n",
    "    return across_assignments, within_assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeed26",
   "metadata": {},
   "source": [
    "## Form assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86644051",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cond_fold_strs = dict()\n",
    "single_cond_fold_strs = dict()\n",
    "for tgt_subj in ps['subjects']:\n",
    "    fish_multi_cond_folds = dict()\n",
    "    fish_single_cond_folds = dict()\n",
    "    for tgt_cond in ps['test_groups']:\n",
    "        trans_subjs = [s_n for s_n in ps['subjects'] if s_n != tgt_subj]\n",
    "        trans_conds = [cond for cond in ps['test_groups']  if cond != tgt_cond]\n",
    "        \n",
    "        multi_cond_folds, single_cond_folds = assign_for_one_tgt_subj(seg_tables=seg_tables, \n",
    "                                                tgt_subj=tgt_subj, \n",
    "                                                tgt_subj_train_group=tgt_cond, \n",
    "                                                train_percentage=ps['train_percentage'], \n",
    "                                                validation_percentage=ps['validation_percentage'],\n",
    "                                                trans_subjs=trans_subjs, \n",
    "                                                trans_train_groups=trans_conds)\n",
    "        \n",
    "        fish_multi_cond_folds[tgt_cond] = multi_cond_folds\n",
    "        fish_single_cond_folds[tgt_cond] = single_cond_folds\n",
    "    \n",
    "    multi_cond_fold_strs[tgt_subj] = fish_multi_cond_folds\n",
    "    single_cond_fold_strs[tgt_subj] = fish_single_cond_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d535e8",
   "metadata": {},
   "source": [
    "Rearrange the fold structures so they are organized so that the fish is the first level and then fold is the second level - this will then allow the fold structures to be used seamlessly with our standard fitting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece7b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_multi_cond_fold_strs = dict()\n",
    "new_single_cond_fold_strs = dict()\n",
    "for tgt_subj in ps['subjects']:\n",
    "    new_multi_cond_fold_strs[tgt_subj] = dict()\n",
    "    new_single_cond_fold_strs[tgt_subj] = dict()\n",
    "    for s_n in ps['subjects']:\n",
    "        new_multi_cond_fold_strs[tgt_subj][s_n] = dict()\n",
    "        new_single_cond_fold_strs[tgt_subj][s_n] = dict()\n",
    "        for tgt_cond in ps['test_groups']:\n",
    "            new_multi_cond_fold_strs[tgt_subj][s_n][tgt_cond] = multi_cond_fold_strs[tgt_subj][tgt_cond][s_n]\n",
    "            new_single_cond_fold_strs[tgt_subj][s_n][tgt_cond] = single_cond_fold_strs[tgt_subj][tgt_cond][s_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdf28b1",
   "metadata": {},
   "source": [
    "## Save fold structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feec9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt_subj in ps['subjects']:\n",
    "    multi_cond_folds = new_multi_cond_fold_strs[tgt_subj]\n",
    "    single_cond_folds = new_single_cond_fold_strs[tgt_subj]\n",
    "    \n",
    "    multi_cond_file_name = ps['save_str'] + '_tgt_' + str(tgt_subj) + '_multi_cond_folds.pkl'\n",
    "    single_cond_file_name = ps['save_str'] + '_tgt_' + str(tgt_subj) + '_single_cond_folds.pkl'\n",
    "    \n",
    "    multi_cond_path = Path(ps['save_folder']) / multi_cond_file_name\n",
    "    single_cond_path = Path(ps['save_folder']) / single_cond_file_name\n",
    "    \n",
    "    with open(multi_cond_path, 'wb') as f:\n",
    "        pickle.dump(multi_cond_folds, f)\n",
    "    with open(single_cond_path, 'wb') as f:\n",
    "        pickle.dump(single_cond_folds, f)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c17b0",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf039e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'omr_f_ns': ['set_13',\n",
       "   'set_51',\n",
       "   'set_87',\n",
       "   'set_48',\n",
       "   'set_2',\n",
       "   'set_45',\n",
       "   'set_4',\n",
       "   'set_36',\n",
       "   'set_77',\n",
       "   'set_73',\n",
       "   'set_43',\n",
       "   'set_7',\n",
       "   'set_24',\n",
       "   'set_75',\n",
       "   'set_106',\n",
       "   'set_97',\n",
       "   'set_58',\n",
       "   'set_62',\n",
       "   'set_11',\n",
       "   'set_76',\n",
       "   'set_5',\n",
       "   'set_81']},\n",
       " 'validation': {'omr_f_ns': ['set_6', 'set_55', 'set_41', 'set_71']},\n",
       " 'test': {'omr_f_ns': ['set_100',\n",
       "   'set_35',\n",
       "   'set_86',\n",
       "   'set_64',\n",
       "   'set_28',\n",
       "   'set_44',\n",
       "   'set_70',\n",
       "   'set_20',\n",
       "   'set_95',\n",
       "   'set_56',\n",
       "   'set_90',\n",
       "   'set_80',\n",
       "   'set_84',\n",
       "   'set_8',\n",
       "   'set_69',\n",
       "   'set_57',\n",
       "   'set_89',\n",
       "   'set_93',\n",
       "   'set_38',\n",
       "   'set_68',\n",
       "   'set_40',\n",
       "   'set_9',\n",
       "   'set_25',\n",
       "   'set_32',\n",
       "   'set_82',\n",
       "   'set_91',\n",
       "   'set_61',\n",
       "   'set_0',\n",
       "   'set_88',\n",
       "   'set_59',\n",
       "   'set_66',\n",
       "   'set_92',\n",
       "   'set_67',\n",
       "   'set_98',\n",
       "   'set_17',\n",
       "   'set_34',\n",
       "   'set_33',\n",
       "   'set_27',\n",
       "   'set_78',\n",
       "   'set_63',\n",
       "   'set_83',\n",
       "   'set_19',\n",
       "   'set_30',\n",
       "   'set_105',\n",
       "   'set_49',\n",
       "   'set_47',\n",
       "   'set_101',\n",
       "   'set_52',\n",
       "   'set_1',\n",
       "   'set_74',\n",
       "   'set_23',\n",
       "   'set_99',\n",
       "   'set_103',\n",
       "   'set_10',\n",
       "   'set_3',\n",
       "   'set_42',\n",
       "   'set_12',\n",
       "   'set_65',\n",
       "   'set_79',\n",
       "   'set_72',\n",
       "   'set_39',\n",
       "   'set_46',\n",
       "   'set_104',\n",
       "   'set_18',\n",
       "   'set_21',\n",
       "   'set_16',\n",
       "   'set_94',\n",
       "   'set_53',\n",
       "   'set_22',\n",
       "   'set_26',\n",
       "   'set_102',\n",
       "   'set_14',\n",
       "   'set_107',\n",
       "   'set_54',\n",
       "   'set_15',\n",
       "   'set_96',\n",
       "   'set_50',\n",
       "   'set_29',\n",
       "   'set_85',\n",
       "   'set_37',\n",
       "   'set_31',\n",
       "   'set_60'],\n",
       "  'omr_l_ns': ['set_0',\n",
       "   'set_1',\n",
       "   'set_2',\n",
       "   'set_3',\n",
       "   'set_4',\n",
       "   'set_5',\n",
       "   'set_6',\n",
       "   'set_7',\n",
       "   'set_8',\n",
       "   'set_9',\n",
       "   'set_10',\n",
       "   'set_11',\n",
       "   'set_12',\n",
       "   'set_13',\n",
       "   'set_14',\n",
       "   'set_15',\n",
       "   'set_16',\n",
       "   'set_17',\n",
       "   'set_18',\n",
       "   'set_19',\n",
       "   'set_20',\n",
       "   'set_21',\n",
       "   'set_22',\n",
       "   'set_23',\n",
       "   'set_24',\n",
       "   'set_25',\n",
       "   'set_26',\n",
       "   'set_27',\n",
       "   'set_28',\n",
       "   'set_29',\n",
       "   'set_30',\n",
       "   'set_31',\n",
       "   'set_32',\n",
       "   'set_33',\n",
       "   'set_34',\n",
       "   'set_35',\n",
       "   'set_36',\n",
       "   'set_37',\n",
       "   'set_38',\n",
       "   'set_39',\n",
       "   'set_40',\n",
       "   'set_41',\n",
       "   'set_42',\n",
       "   'set_43',\n",
       "   'set_44',\n",
       "   'set_45',\n",
       "   'set_46',\n",
       "   'set_47',\n",
       "   'set_48',\n",
       "   'set_49',\n",
       "   'set_50',\n",
       "   'set_51',\n",
       "   'set_52',\n",
       "   'set_53',\n",
       "   'set_54',\n",
       "   'set_55',\n",
       "   'set_56',\n",
       "   'set_57',\n",
       "   'set_58',\n",
       "   'set_59',\n",
       "   'set_60',\n",
       "   'set_61',\n",
       "   'set_62',\n",
       "   'set_63',\n",
       "   'set_64',\n",
       "   'set_65',\n",
       "   'set_66',\n",
       "   'set_67',\n",
       "   'set_68',\n",
       "   'set_69',\n",
       "   'set_70',\n",
       "   'set_71',\n",
       "   'set_72',\n",
       "   'set_73',\n",
       "   'set_74',\n",
       "   'set_75',\n",
       "   'set_76',\n",
       "   'set_77',\n",
       "   'set_78',\n",
       "   'set_79',\n",
       "   'set_80',\n",
       "   'set_81',\n",
       "   'set_82',\n",
       "   'set_83',\n",
       "   'set_84',\n",
       "   'set_85',\n",
       "   'set_86',\n",
       "   'set_87',\n",
       "   'set_88',\n",
       "   'set_89',\n",
       "   'set_90',\n",
       "   'set_91',\n",
       "   'set_92',\n",
       "   'set_93',\n",
       "   'set_94',\n",
       "   'set_95',\n",
       "   'set_96',\n",
       "   'set_97',\n",
       "   'set_98',\n",
       "   'set_99',\n",
       "   'set_100',\n",
       "   'set_101',\n",
       "   'set_102',\n",
       "   'set_103',\n",
       "   'set_104',\n",
       "   'set_105',\n",
       "   'set_106',\n",
       "   'set_107'],\n",
       "  'omr_r_ns': ['set_0',\n",
       "   'set_1',\n",
       "   'set_2',\n",
       "   'set_3',\n",
       "   'set_4',\n",
       "   'set_5',\n",
       "   'set_6',\n",
       "   'set_7',\n",
       "   'set_8',\n",
       "   'set_9',\n",
       "   'set_10',\n",
       "   'set_11',\n",
       "   'set_12',\n",
       "   'set_13',\n",
       "   'set_14',\n",
       "   'set_15',\n",
       "   'set_16',\n",
       "   'set_17',\n",
       "   'set_18',\n",
       "   'set_19',\n",
       "   'set_20',\n",
       "   'set_21',\n",
       "   'set_22',\n",
       "   'set_23',\n",
       "   'set_24',\n",
       "   'set_25',\n",
       "   'set_26',\n",
       "   'set_27',\n",
       "   'set_28',\n",
       "   'set_29',\n",
       "   'set_30',\n",
       "   'set_31',\n",
       "   'set_32',\n",
       "   'set_33',\n",
       "   'set_34',\n",
       "   'set_35',\n",
       "   'set_36',\n",
       "   'set_37',\n",
       "   'set_38',\n",
       "   'set_39',\n",
       "   'set_40',\n",
       "   'set_41',\n",
       "   'set_42',\n",
       "   'set_43',\n",
       "   'set_44',\n",
       "   'set_45',\n",
       "   'set_46',\n",
       "   'set_47',\n",
       "   'set_48',\n",
       "   'set_49',\n",
       "   'set_50',\n",
       "   'set_51',\n",
       "   'set_52',\n",
       "   'set_53',\n",
       "   'set_54',\n",
       "   'set_55',\n",
       "   'set_56',\n",
       "   'set_57',\n",
       "   'set_58',\n",
       "   'set_59',\n",
       "   'set_60',\n",
       "   'set_61',\n",
       "   'set_62',\n",
       "   'set_63',\n",
       "   'set_64',\n",
       "   'set_65',\n",
       "   'set_66',\n",
       "   'set_67',\n",
       "   'set_68',\n",
       "   'set_69',\n",
       "   'set_70',\n",
       "   'set_71',\n",
       "   'set_72',\n",
       "   'set_73',\n",
       "   'set_74',\n",
       "   'set_75',\n",
       "   'set_76',\n",
       "   'set_77',\n",
       "   'set_78',\n",
       "   'set_79',\n",
       "   'set_80',\n",
       "   'set_81',\n",
       "   'set_82',\n",
       "   'set_83',\n",
       "   'set_84',\n",
       "   'set_85',\n",
       "   'set_86',\n",
       "   'set_87',\n",
       "   'set_88',\n",
       "   'set_89',\n",
       "   'set_90',\n",
       "   'set_91',\n",
       "   'set_92',\n",
       "   'set_93',\n",
       "   'set_94',\n",
       "   'set_95',\n",
       "   'set_96',\n",
       "   'set_97',\n",
       "   'set_98',\n",
       "   'set_99',\n",
       "   'set_100',\n",
       "   'set_101',\n",
       "   'set_102',\n",
       "   'set_103',\n",
       "   'set_104',\n",
       "   'set_105',\n",
       "   'set_106',\n",
       "   'set_107']}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_multi_cond_fold_strs[8][8]['omr_f_ns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658c6bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/real_data/ac_an_tgt_11_multi_cond_folds.pkl')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cond_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0feac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
