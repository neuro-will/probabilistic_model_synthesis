{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conceptual-season",
   "metadata": {},
   "source": [
    "We simulate a small amount of data from many individual FA models and then we fit the models together and alone and measure the benefit (in terms of the likelihood of held-out test data) of fitting the models together vs. fitting them together. \n",
    "\n",
    "\n",
    "For comparison, when fitting FA models individually we use a standard FA fitting package to estimate point estimates for model parameters.  When evaluating models that have been fit together, we use the modes of posterior distributions as point estimates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "available-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "empty-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from probabilistic_model_synthesis.fa import FAMdl\n",
    "from probabilistic_model_synthesis.fa import Fitter\n",
    "from probabilistic_model_synthesis.fa import generate_basic_posteriors\n",
    "from probabilistic_model_synthesis.fa import generate_simple_prior_collection\n",
    "from probabilistic_model_synthesis.fa import VICollection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-documentary",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accompanied-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of individuals we simulate observing data from \n",
    "n_individuals = 10\n",
    "\n",
    "# Range of the number of variables we observe from each individual - the actual number of variables we observe from an\n",
    "# individual will be pulled uniformly from this range (inclusive)\n",
    "n_var_range = [100, 120]\n",
    "\n",
    "# Range of the number of samples we observe for fittig from each individual - the actual number we observe \n",
    "# from each individual will be unformly from this range (inclusive)\n",
    "n_fitting_smps_range = [10, 15]\n",
    "\n",
    "# Number of latent variables in the model\n",
    "n_latent_vars = 3\n",
    "\n",
    "# Number of samples we generate when testing each model\n",
    "n_test_smps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-account",
   "metadata": {},
   "source": [
    "## Create the true prior distributions that relate parameters in the model to variable (e.g., neuron) properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interpreted-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_priors = generate_simple_prior_collection(n_prop_vars=2, n_latent_vars=n_latent_vars, \n",
    "                                               lm_mn_w_init_std=1.0, lm_std_w_init_std=.1,\n",
    "                                               mn_mn_w_init_std=1.0, mn_std_w_init_std=1.0,\n",
    "                                               psi_conc_f_w_init_std=2.0, psi_rate_f_w_init_std=1.0, \n",
    "                                               psi_conc_bias_mn=10.0, psi_rate_bias_mn=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-trader",
   "metadata": {},
   "source": [
    "## Generate properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tribal-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_n_vars = np.random.randint(n_var_range[0], n_var_range[1]+1, n_individuals)\n",
    "ind_n_smps = np.random.randint(n_fitting_smps_range[0], n_fitting_smps_range[1]+1, n_individuals)\n",
    "ind_props = [torch.rand(size=[n_vars,2]) for n_vars in ind_n_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-optimum",
   "metadata": {},
   "source": [
    "## Generate true FA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continuing-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ind_true_fa_mdls = [FAMdl(lm=true_priors.lm_prior.sample(props), mn=true_priors.mn_prior.sample(props).squeeze(), \n",
    "                           psi=(true_priors.psi_prior.sample(props).squeeze()))\n",
    "                        for props in ind_props]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-broadway",
   "metadata": {},
   "source": [
    "## Generate data for fitting from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "considered-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ind_train_data = [mdl.sample(n_smps) for n_smps, mdl in zip(ind_n_smps, ind_true_fa_mdls)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-proposal",
   "metadata": {},
   "source": [
    "## Fit FA models together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proof-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_priors = generate_simple_prior_collection(n_prop_vars=2, n_latent_vars=n_latent_vars)\n",
    "fit_posteriors = generate_basic_posteriors(n_obs_vars=ind_n_vars, n_smps=ind_n_smps, n_latent_vars=n_latent_vars)\n",
    "\n",
    "fit_mdls = [FAMdl(lm=None, mn=None, psi=None) for i in range(n_individuals)]\n",
    "\n",
    "vi_collections = [VICollection(data=data_i[1], props=props_i, mdl=mdl_i, posteriors=posteriors_i) \n",
    "                  for data_i, props_i,mdl_i, posteriors_i in zip(ind_train_data, ind_props, fit_mdls, fit_posteriors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "protected-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = Fitter(vi_collections=vi_collections, priors=fit_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "legal-waterproof",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== EPOCH 0 COMPLETE ===========\n",
      "Obj: 1.18e+05\n",
      "----------------------------------------\n",
      "NELL: 1.00e+04, 1.55e+04, 9.93e+03, 1.08e+04, 1.78e+04, 1.35e+04, 1.00e+04, 8.86e+03, 1.33e+04, 1.32e+04\n",
      "Latent KL: 3.23e-01, 3.23e-01, 2.98e-01, 3.23e-01, 3.73e-01, 4.78e-01, 2.98e-01, 2.73e-01, 3.53e-01, 3.48e-01\n",
      "LM KL: 8.79e+02, 8.64e+02, 8.69e+02, 7.46e+02, 6.62e+02, 6.54e+02, 7.78e+02, 7.96e+02, 8.31e+02, 9.31e+02\n",
      "Mn KL: 1.85e+02, 2.13e+02, 2.06e+02, 1.96e+02, 2.05e+02, 2.12e+02, 1.87e+02, 1.85e+02, 2.12e+02, 2.05e+02\n",
      "Psi KL: 2.45e+01, 2.75e+01, 2.64e+01, 2.56e+01, 2.65e+01, 2.71e+01, 2.39e+01, 2.39e+01, 2.62e+01, 2.74e+01\n",
      "----------------------------------------\n",
      "LR: 0.1\n",
      "\n",
      "=========== EPOCH 100 COMPLETE ===========\n",
      "Obj: 3.11e+04\n",
      "----------------------------------------\n",
      "NELL: 2.60e+03, 2.90e+03, 2.62e+03, 3.04e+03, 3.20e+03, 2.97e+03, 2.37e+03, 2.15e+03, 2.96e+03, 3.08e+03\n",
      "Latent KL: 6.72e+01, 7.57e+01, 5.89e+01, 6.35e+01, 8.10e+01, 5.60e+01, 4.84e+01, 5.15e+01, 5.68e+01, 5.62e+01\n",
      "LM KL: 6.33e+01, 7.67e+01, 5.05e+01, 7.78e+01, 1.42e+02, 8.41e+01, 4.40e+01, 8.31e+01, 7.24e+01, 1.69e+02\n",
      "Mn KL: 1.41e+02, 1.61e+02, 1.47e+02, 1.56e+02, 1.64e+02, 1.48e+02, 1.29e+02, 1.28e+02, 1.63e+02, 1.54e+02\n",
      "Psi KL: 1.51e+01, 1.70e+01, 2.09e+01, 1.74e+01, 2.06e+01, 1.91e+01, 1.43e+01, 1.46e+01, 1.55e+01, 2.26e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 200 COMPLETE ===========\n",
      "Obj: 3.02e+04\n",
      "----------------------------------------\n",
      "NELL: 2.58e+03, 2.90e+03, 2.61e+03, 2.96e+03, 3.15e+03, 2.88e+03, 2.34e+03, 2.11e+03, 2.87e+03, 3.03e+03\n",
      "Latent KL: 6.58e+01, 7.14e+01, 5.68e+01, 5.85e+01, 7.47e+01, 6.08e+01, 5.46e+01, 4.73e+01, 5.96e+01, 6.35e+01\n",
      "LM KL: 3.29e+01, 4.02e+01, 2.43e+01, 4.10e+01, 1.03e+02, 5.32e+01, 1.81e+01, 4.55e+01, 4.23e+01, 1.21e+02\n",
      "Mn KL: 1.37e+02, 1.54e+02, 1.40e+02, 1.50e+02, 1.64e+02, 1.44e+02, 1.24e+02, 1.22e+02, 1.52e+02, 1.53e+02\n",
      "Psi KL: 1.48e+01, 1.77e+01, 2.18e+01, 1.84e+01, 2.09e+01, 2.07e+01, 1.39e+01, 1.54e+01, 1.66e+01, 2.23e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 300 COMPLETE ===========\n",
      "Obj: 3.00e+04\n",
      "----------------------------------------\n",
      "NELL: 2.59e+03, 2.88e+03, 2.57e+03, 2.96e+03, 3.14e+03, 2.92e+03, 2.31e+03, 2.09e+03, 2.90e+03, 3.05e+03\n",
      "Latent KL: 6.28e+01, 6.73e+01, 5.53e+01, 5.91e+01, 7.40e+01, 6.24e+01, 4.92e+01, 4.75e+01, 5.59e+01, 6.03e+01\n",
      "LM KL: 2.47e+01, 2.96e+01, 1.80e+01, 3.00e+01, 8.12e+01, 4.37e+01, 1.29e+01, 3.00e+01, 2.92e+01, 8.32e+01\n",
      "Mn KL: 1.38e+02, 1.58e+02, 1.43e+02, 1.49e+02, 1.69e+02, 1.47e+02, 1.27e+02, 1.22e+02, 1.56e+02, 1.53e+02\n",
      "Psi KL: 1.56e+01, 1.90e+01, 2.32e+01, 1.95e+01, 2.22e+01, 2.24e+01, 1.45e+01, 1.74e+01, 1.75e+01, 2.40e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 400 COMPLETE ===========\n",
      "Obj: 2.97e+04\n",
      "----------------------------------------\n",
      "NELL: 2.58e+03, 2.86e+03, 2.57e+03, 2.96e+03, 3.12e+03, 2.90e+03, 2.31e+03, 2.13e+03, 2.87e+03, 3.03e+03\n",
      "Latent KL: 6.02e+01, 6.11e+01, 5.01e+01, 5.65e+01, 6.69e+01, 6.10e+01, 4.89e+01, 4.99e+01, 5.65e+01, 6.05e+01\n",
      "LM KL: 1.85e+01, 2.69e+01, 1.61e+01, 2.18e+01, 6.03e+01, 3.35e+01, 9.47e+00, 2.36e+01, 2.18e+01, 5.22e+01\n",
      "Mn KL: 1.39e+02, 1.62e+02, 1.45e+02, 1.52e+02, 1.73e+02, 1.51e+02, 1.26e+02, 1.23e+02, 1.59e+02, 1.57e+02\n",
      "Psi KL: 1.70e+01, 2.00e+01, 2.48e+01, 2.09e+01, 2.35e+01, 2.40e+01, 1.56e+01, 1.90e+01, 1.85e+01, 2.49e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 500 COMPLETE ===========\n",
      "Obj: 2.99e+04\n",
      "----------------------------------------\n",
      "NELL: 2.56e+03, 2.82e+03, 2.60e+03, 2.95e+03, 3.11e+03, 2.92e+03, 2.31e+03, 2.09e+03, 2.89e+03, 3.03e+03\n",
      "Latent KL: 6.15e+01, 6.27e+01, 5.29e+01, 5.39e+01, 6.99e+01, 5.72e+01, 4.77e+01, 4.22e+01, 5.73e+01, 5.94e+01\n",
      "LM KL: 1.50e+01, 2.68e+01, 1.63e+01, 1.79e+01, 4.44e+01, 2.97e+01, 9.88e+00, 1.83e+01, 1.84e+01, 3.29e+01\n",
      "Mn KL: 1.38e+02, 1.64e+02, 1.46e+02, 1.52e+02, 1.73e+02, 1.51e+02, 1.26e+02, 1.25e+02, 1.60e+02, 1.56e+02\n",
      "Psi KL: 1.80e+01, 2.09e+01, 2.60e+01, 2.11e+01, 2.50e+01, 2.60e+01, 1.63e+01, 2.05e+01, 1.96e+01, 2.58e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 600 COMPLETE ===========\n",
      "Obj: 2.97e+04\n",
      "----------------------------------------\n",
      "NELL: 2.54e+03, 2.84e+03, 2.57e+03, 2.92e+03, 3.10e+03, 2.89e+03, 2.30e+03, 2.08e+03, 2.86e+03, 3.02e+03\n",
      "Latent KL: 5.74e+01, 5.52e+01, 4.93e+01, 5.72e+01, 7.18e+01, 5.71e+01, 4.51e+01, 4.30e+01, 5.79e+01, 5.61e+01\n",
      "LM KL: 1.46e+01, 2.66e+01, 2.11e+01, 1.65e+01, 3.53e+01, 2.82e+01, 1.17e+01, 1.89e+01, 1.75e+01, 2.23e+01\n",
      "Mn KL: 1.38e+02, 1.65e+02, 1.44e+02, 1.55e+02, 1.74e+02, 1.50e+02, 1.29e+02, 1.25e+02, 1.62e+02, 1.58e+02\n",
      "Psi KL: 1.86e+01, 2.24e+01, 2.66e+01, 2.20e+01, 2.64e+01, 2.74e+01, 1.73e+01, 2.11e+01, 2.02e+01, 2.69e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 700 COMPLETE ===========\n",
      "Obj: 2.96e+04\n",
      "----------------------------------------\n",
      "NELL: 2.57e+03, 2.88e+03, 2.55e+03, 2.94e+03, 3.13e+03, 2.89e+03, 2.28e+03, 2.09e+03, 2.88e+03, 3.01e+03\n",
      "Latent KL: 5.70e+01, 6.13e+01, 5.05e+01, 5.23e+01, 7.31e+01, 5.93e+01, 4.42e+01, 4.24e+01, 5.34e+01, 5.89e+01\n",
      "LM KL: 1.35e+01, 2.51e+01, 2.24e+01, 1.48e+01, 3.02e+01, 2.55e+01, 1.27e+01, 1.80e+01, 1.69e+01, 1.74e+01\n",
      "Mn KL: 1.42e+02, 1.65e+02, 1.44e+02, 1.53e+02, 1.74e+02, 1.50e+02, 1.31e+02, 1.26e+02, 1.60e+02, 1.59e+02\n",
      "Psi KL: 1.89e+01, 2.28e+01, 2.75e+01, 2.34e+01, 2.76e+01, 2.81e+01, 1.79e+01, 2.20e+01, 2.15e+01, 2.78e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 800 COMPLETE ===========\n",
      "Obj: 2.96e+04\n",
      "----------------------------------------\n",
      "NELL: 2.55e+03, 2.86e+03, 2.57e+03, 2.92e+03, 3.10e+03, 2.90e+03, 2.31e+03, 2.09e+03, 2.86e+03, 3.00e+03\n",
      "Latent KL: 5.64e+01, 5.59e+01, 4.70e+01, 5.26e+01, 7.22e+01, 6.20e+01, 4.95e+01, 4.03e+01, 5.33e+01, 5.42e+01\n",
      "LM KL: 1.59e+01, 2.42e+01, 2.08e+01, 1.47e+01, 2.50e+01, 2.26e+01, 1.48e+01, 1.84e+01, 1.49e+01, 1.48e+01\n",
      "Mn KL: 1.41e+02, 1.66e+02, 1.45e+02, 1.54e+02, 1.73e+02, 1.50e+02, 1.30e+02, 1.23e+02, 1.59e+02, 1.60e+02\n",
      "Psi KL: 2.01e+01, 2.42e+01, 2.83e+01, 2.40e+01, 2.85e+01, 2.91e+01, 1.82e+01, 2.26e+01, 2.18e+01, 2.90e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n",
      "\n",
      "=========== EPOCH 900 COMPLETE ===========\n",
      "Obj: 2.96e+04\n",
      "----------------------------------------\n",
      "NELL: 2.53e+03, 2.85e+03, 2.55e+03, 2.92e+03, 3.08e+03, 2.90e+03, 2.31e+03, 2.10e+03, 2.90e+03, 3.02e+03\n",
      "Latent KL: 5.52e+01, 5.44e+01, 4.38e+01, 5.57e+01, 6.59e+01, 5.58e+01, 4.58e+01, 4.36e+01, 5.09e+01, 5.33e+01\n",
      "LM KL: 1.74e+01, 1.93e+01, 1.79e+01, 1.48e+01, 2.26e+01, 1.88e+01, 1.39e+01, 1.81e+01, 1.37e+01, 1.27e+01\n",
      "Mn KL: 1.41e+02, 1.65e+02, 1.44e+02, 1.54e+02, 1.74e+02, 1.48e+02, 1.29e+02, 1.23e+02, 1.60e+02, 1.59e+02\n",
      "Psi KL: 2.00e+01, 2.46e+01, 2.89e+01, 2.46e+01, 3.02e+01, 3.00e+01, 1.83e+01, 2.32e+01, 2.30e+01, 2.89e+01\n",
      "----------------------------------------\n",
      "LR: 0.010000000000000002\n"
     ]
    }
   ],
   "source": [
    "logs = [fitter.fit(1000, milestones=[100], update_int=100, init_lr=.1, skip_lm_kl=False, \n",
    "                 skip_mn_kl=False, skip_psi_kl=False) for fit_r in range(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-subscription",
   "metadata": {},
   "source": [
    "## Fit FA models individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "breeding-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "alone_models = [None]*n_individuals\n",
    "for ind_i in range(n_individuals):\n",
    "    mdl = sklearn.decomposition.FactorAnalysis(n_components=n_latent_vars)\n",
    "    mdl.fit(ind_train_data[ind_i][1].numpy())\n",
    "    alone_models[ind_i] = mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-default",
   "metadata": {},
   "source": [
    "## Measure performance of the fit models on new test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "judicial-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ind_test_data = [mdl.sample(n_test_smps) for mdl in ind_true_fa_mdls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "instant-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_test_ll = [None]*n_individuals\n",
    "with torch.no_grad():\n",
    "    eval_mdl = FAMdl() # Model object we ust just for evaluation \n",
    "    for ind_i in range(n_individuals):\n",
    "    \n",
    "        mdl_test_data = ind_test_data[ind_i][1]\n",
    "        \n",
    "        # Calculate log-likelihood using model fit alone\n",
    "        \n",
    "        alone_lm = torch.tensor(alone_models[ind_i].components_.transpose())\n",
    "        alone_mn = torch.tensor(alone_models[ind_i].mean_)\n",
    "        alone_psi = torch.tensor(alone_models[ind_i].noise_variance_)\n",
    "    \n",
    "        alone_ll = torch.sum(eval_mdl.log_prob(x=mdl_test_data, lm=alone_lm, mn=alone_mn, psi=alone_psi))\n",
    "        alone_ll = (alone_ll/n_test_smps).numpy().item()\n",
    "        \n",
    "        # Calculate log-likelihood using model fit with the other models\n",
    "        \n",
    "        comb_lm = vi_collections[ind_i].posteriors.lm_post(ind_props[ind_i])\n",
    "        comb_mn = vi_collections[ind_i].posteriors.mn_post(ind_props[ind_i]).squeeze()\n",
    "        comb_psi = vi_collections[ind_i].posteriors.psi_post.mode(ind_props[ind_i]).squeeze()\n",
    "                             \n",
    "        comb_ll = torch.sum(eval_mdl.log_prob(x=mdl_test_data, lm=comb_lm, mn=comb_mn, psi=comb_psi))\n",
    "        comb_ll = (comb_ll/n_test_smps).numpy().item()\n",
    "        \n",
    "        ind_test_ll[ind_i] = {'alone': alone_ll, 'comb': comb_ll}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "solved-columbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'alone': -260.86021631709866, 'comb': -205.94659423828125},\n",
       " {'alone': -270.22096438944305, 'comb': -227.8758087158203},\n",
       " {'alone': -289.7494554399177, 'comb': -219.4298858642578},\n",
       " {'alone': -262.46040206997964, 'comb': -214.86265563964844},\n",
       " {'alone': -253.61787275397472, 'comb': -215.7176513671875},\n",
       " {'alone': -285.1326063212853, 'comb': -231.3709716796875},\n",
       " {'alone': -254.52365214430822, 'comb': -195.76116943359375},\n",
       " {'alone': -268.9499231095329, 'comb': -200.334228515625},\n",
       " {'alone': -283.38469520116894, 'comb': -218.39013671875},\n",
       " {'alone': -276.2552528035746, 'comb': -220.1079864501953}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_test_ll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
