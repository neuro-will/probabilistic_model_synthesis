{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558a7f14",
   "metadata": {},
   "source": [
    "A notebook for visualizing the results of the script fit_one_dim_synthesis_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d41f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1feeae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janelia_core.ml.wandering_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5e261b0463a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprobabilistic_model_synthesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_nonlinear_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPriorCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprobabilistic_model_synthesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_nonlinear_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVICollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprobabilistic_model_synthesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mefficient_cone_and_projected_interval_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/code/probabilistic_model_synthesis/probabilistic_model_synthesis/simulation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjanelia_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_distributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCondMatrixProductDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjanelia_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_distributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCondGaussianDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjanelia_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandering_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSumOfBumpFcns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janelia_core.ml.wandering_modules'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from janelia_core.math.basic_functions import list_grid_pts\n",
    "from janelia_core.ml.fitting import match_torch_module\n",
    "from janelia_core.math.basic_functions import bound\n",
    "from janelia_core.math.basic_functions import pts_in_arc\n",
    "from janelia_core.ml.utils import list_torch_devices\n",
    "from janelia_core.stats.regression import corr\n",
    "from janelia_core.stats.regression import r_squared\n",
    "\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import approximate_elbo\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import eval_check_point_perf\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import Fitter\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import load_check_points\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import predict\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import PosteriorCollection\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import PriorCollection\n",
    "from probabilistic_model_synthesis.gaussian_nonlinear_regression import VICollection\n",
    "from probabilistic_model_synthesis.simulation import efficient_cone_and_projected_interval_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ed5a6",
   "metadata": {},
   "source": [
    "## Parameters go here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of folder with the results we should visualize\n",
    "rs_folder = r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/simulation/gnlr/no_props/with_props'\n",
    "\n",
    "# File holding fitting results\n",
    "rs_file = 'with_props.pt'\n",
    "\n",
    "# Sub-folders holding check points\n",
    "combined_cp_folder = 'comb_cps'\n",
    "single_cp_folder = 'single_cps'\n",
    "\n",
    "# Type of results we should look at - combined or single\n",
    "rs_type = 'combined'\n",
    "\n",
    "# True if we should look at fitting logs\n",
    "vis_fit_logs = True\n",
    "\n",
    "# Index of the example system we should visualize results for\n",
    "ex_s_i = 1 # 1 is a good example\n",
    "\n",
    "# Number of samples we use for validation data when performing early stopping\n",
    "n_validation_smps = 1000\n",
    "\n",
    "# Number of samples we generate for evaluating model performance\n",
    "n_eval_smps = 10000\n",
    "\n",
    "# Location we should save plots to - if this folder doesn't exist we will create it \n",
    "save_loc = r'/groups/bishop/bishoplab/projects/probabilistic_model_synthesis/results/simulation/gnlr/no_props/with_props'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce6647",
   "metadata": {},
   "source": [
    "## Determine if GPU is availalbe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_devices, gpu_available = list_torch_devices()\n",
    "if gpu_available:\n",
    "    compute_device = compute_devices[0]\n",
    "else:\n",
    "    compute_device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5478fcd",
   "metadata": {},
   "source": [
    "## Load results and prepare what we need for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_file_path = pathlib.Path(rs_folder) / rs_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d357ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = torch.load(rs_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d88ffe",
   "metadata": {},
   "source": [
    "Pull out constants, paths, etc we will need below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ex_systems = rs['ps']['n_ex_systems']\n",
    "\n",
    "subj_props = rs['ind_props']\n",
    "fit_ps = rs['ps']\n",
    "single_fit_inds = rs['ps']['single_fit_inds']\n",
    "n_single_fit_systems = len(single_fit_inds)\n",
    "\n",
    "combined_cp_folder = pathlib.Path(rs_folder) / combined_cp_folder\n",
    "single_cp_folder = pathlib.Path(rs_folder) / single_cp_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424adae",
   "metadata": {},
   "source": [
    "## Apply retroactive early stopping to synthesized models and for model fit individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a13104",
   "metadata": {},
   "source": [
    "Generate validation data for each subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccaf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [None]*n_ex_systems\n",
    "for s_i in range(n_ex_systems):\n",
    "    \n",
    "    # We generate data with the same active neurons and behevioral range as was in the training data for\n",
    "    # this subject\n",
    "    with torch.no_grad():\n",
    "        x_i_validation = efficient_cone_and_projected_interval_sample(n_smps=n_validation_smps,\n",
    "                                                                        locs=rs['ind_props'][s_i],\n",
    "                                                                        ctr=torch.tensor([.5, .5]),\n",
    "                                                                        ang_range=rs['ang_ranges'][s_i],\n",
    "                                                                        w=rs['ind_true_mdls'][s_i].w.detach(),\n",
    "                                                                        interval=rs['ind_intervals'][s_i],\n",
    "                                                                        big_std=1.0,\n",
    "                                                                        small_std=0,\n",
    "                                                                        device=compute_device)\n",
    "    \n",
    "        # Before generating y data, move model to GPU (if possible)\n",
    "        rs['ind_true_mdls'][s_i].to(compute_device)\n",
    "        y_i_validation = rs['ind_true_mdls'][s_i].sample(x=x_i_validation)\n",
    "        \n",
    "        # Move data back to cpu to save GPU memory\n",
    "        x_i_validation = x_i_validation.to('cpu')\n",
    "        y_i_validation = y_i_validation.to('cpu')\n",
    "        \n",
    "        eval_data[s_i] = (x_i_validation, y_i_validation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f62f2f",
   "metadata": {},
   "source": [
    "Evaluate check point performance for the synthesized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00eab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_sp_cps, comb_sp_cp_epochs = load_check_points(cp_dir=combined_cp_folder, cp_str='cp_sp')\n",
    "comb_ip_cps, comb_ip_cp_epochs = load_check_points(cp_dir=combined_cp_folder, cp_str='cp_ip')\n",
    "\n",
    "comb_sp_perf = eval_check_point_perf(cps=comb_sp_cps, eval_data=eval_data, subj_props=subj_props, \n",
    "                                     eval_device=compute_device)\n",
    "\n",
    "comb_ip_perf = eval_check_point_perf(cps=comb_ip_cps, eval_data=eval_data, subj_props=subj_props, \n",
    "                                     eval_device=compute_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3576a6",
   "metadata": {},
   "source": [
    "Define a smaller helper function for plotting check point performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cp_perf(plt_epochs, perf_vls, ax):\n",
    "    \n",
    "    n_subjs = perf_vls.shape[1]\n",
    "    for s_i in range(n_subjs):\n",
    "        ax.plot(plt_epochs, perf_vls[:, s_i])\n",
    "    \n",
    "    if n_subjs > 1:\n",
    "        ax.plot(plt_epochs, np.mean(perf_vls, axis=1), 'k-', linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b3ec5",
   "metadata": {},
   "source": [
    "Plot performance across check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,2,1)\n",
    "plot_cp_perf(comb_sp_cp_epochs, comb_sp_perf, ax)\n",
    "plt.ylim([-1, 1])\n",
    "plt.title('Combined SP CP Perf')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "plot_cp_perf(comb_ip_cp_epochs, comb_ip_perf, ax)\n",
    "plt.ylim([-1, 1])\n",
    "plt.title('Combined IP CP Perf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53815fd6",
   "metadata": {},
   "source": [
    "Pick the best check point for the synthesized models\n",
    "\n",
    "We pick only check points for models with individual posteriors (as we consider fitting with shared posteriors to be an initiliazation step).  We pick one check point for all subjects - as all models are synthesized together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb_ip_cp_ind = np.argmax(np.mean(comb_ip_perf, axis=1))\n",
    "\n",
    "comb_vi_colls = [VICollection.from_checkpoint(comb_ip_cps[best_comb_ip_cp_ind]['vi_collections'][s_i])\n",
    "                 for s_i in range(n_ex_systems)]\n",
    "\n",
    "comb_priors = PriorCollection.from_checkpoint(comb_ip_cps[best_comb_ip_cp_ind]['priors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bba1f6",
   "metadata": {},
   "source": [
    "Evaluate performance for the models fit to individual systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5fff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_cp_perf = [None]*n_single_fit_systems\n",
    "for i, s_i in enumerate(single_fit_inds):\n",
    "    \n",
    "    cp_folder_i = single_cp_folder / ('s_' + str(s_i))\n",
    "\n",
    "\n",
    "    single_sp_cps_i, single_sp_cp_epochs_i = load_check_points(cp_dir=cp_folder_i, cp_str='cp_sp')\n",
    "    single_ip_cps_i, single_ip_cp_epochs_i = load_check_points(cp_dir=cp_folder_i, cp_str='cp_ip')\n",
    "\n",
    "    single_sp_perf_i = eval_check_point_perf(cps=single_sp_cps_i, eval_data=[eval_data[s_i]], \n",
    "                                             subj_props=[subj_props[s_i]], \n",
    "                                             eval_device=compute_device)\n",
    "\n",
    "    single_ip_perf_i = eval_check_point_perf(cps=single_ip_cps_i, eval_data=[eval_data[s_i]], \n",
    "                                             subj_props=[subj_props[s_i]], \n",
    "                                             eval_device=compute_device)\n",
    "    \n",
    "    single_cp_perf[i] = {'sp_epochs': single_sp_cp_epochs_i, 'sp_perf': single_sp_perf_i, \n",
    "                           'ip_epochs': single_ip_cp_epochs_i, 'ip_perf': single_ip_perf_i, \n",
    "                            'sp_cps': single_sp_cps_i, 'ip_cps': single_ip_cps_i}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df89dbe",
   "metadata": {},
   "source": [
    "Plot performance across check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,2,1)\n",
    "for i in range(n_single_fit_systems):\n",
    "    plot_cp_perf(single_cp_perf[i]['sp_epochs'], single_cp_perf[i]['sp_perf'], ax)\n",
    "plt.ylim([-1, 1])\n",
    "plt.title('Individual SP CP Perf')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "for i in range(n_ex_systems):\n",
    "    plot_cp_perf(single_cp_perf[i]['ip_epochs'], single_cp_perf[i]['ip_perf'], ax)\n",
    "plt.ylim([-1, 1])\n",
    "plt.title('Individual IP CP Perf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209499a",
   "metadata": {},
   "source": [
    "Pick the best check point for each model fit to an example system in isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9795b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_fits = [None]*n_single_fit_systems\n",
    "for i in range(n_single_fit_systems):\n",
    "    best_cp_ind = np.argmax(single_cp_perf[i]['ip_perf'])\n",
    "    \n",
    "    single_fits[i] = {'vi_coll': VICollection.from_checkpoint(\n",
    "         single_cp_perf[i]['ip_cps'][best_cp_ind]['vi_collections'][0]), \n",
    "                      'priors': PriorCollection.from_checkpoint(single_cp_perf[i]['ip_cps'][best_cp_ind]['priors'])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71857cf0",
   "metadata": {},
   "source": [
    "## Pick key quantities we need when showing results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0858a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ex_i = np.argwhere(np.asarray(single_fit_inds) == ex_s_i).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe0bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the domain of the shared m function\n",
    "min_interval = np.min(np.asarray(rs['ind_intervals']))\n",
    "max_interval = np.max(np.asarray(rs['ind_intervals']))\n",
    "min_l_range = min_interval*fit_ps['s_in']\n",
    "max_l_range = max_interval*fit_ps['s_in']\n",
    "\n",
    "# Pull out neurons, their properties and true weights for the example system we use for visualization \n",
    "ex_active_neurons = np.asarray((torch.var(rs['ind_data'][ex_s_i][0], dim=0) > 1E-10).tolist())\n",
    "ex_neuron_props = rs['ind_props'][ex_s_i]\n",
    "ex_true_neuron_weights = rs['ind_true_mdls'][ex_s_i].w.detach().cpu().numpy()\n",
    "\n",
    "# Pull out true CPD\n",
    "true_priors = rs['true_priors']\n",
    "\n",
    "if rs_type == 'combined':\n",
    "    # The fit logs we plot\n",
    "    fit_logs = rs['comb_fit_rs']['ip']['logs']\n",
    "    # The range the shared function should be learnable over \n",
    "    m_fit_range = torch.tensor([[min_l_range], [max_l_range]])\n",
    "    # The fit shared function\n",
    "    m_fit = comb_vi_colls[0].mdl.m\n",
    "    # The posterior mean over neuron weights for the example system\n",
    "    ex_w_fit_mn = comb_vi_colls[ex_s_i].posteriors.w_post(ex_neuron_props).detach().cpu().numpy()\n",
    "    # The fit CPD\n",
    "    fit_priors = comb_priors\n",
    "\n",
    "elif rs_type == 'single':\n",
    "    # The fit logs we plot\n",
    "    fit_logs = rs['single_fit_rs'][single_ex_i]['ip']['logs']\n",
    "    # The range the shared function should be learnable over \n",
    "    m_fit_range = torch.tensor([[rs['ind_intervals'][ex_s_i][0]], [rs['ind_intervals'][ex_s_i][1]]], dtype=torch.float32)*fit_ps['s_in']\n",
    "    # The fit shared function\n",
    "    m_fit = single_fits[single_ex_i]['vi_coll'].mdl.m\n",
    "    # The posterior mean over neuron weights for the example system\n",
    "    ex_w_fit_mn = single_fits[single_ex_i]['vi_coll'].posteriors.w_post(ex_neuron_props).detach().cpu().numpy()\n",
    "    # The fit CPD\n",
    "    fit_priors = single_fits[single_ex_i]['priors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709b92a",
   "metadata": {},
   "source": [
    "## Examine fitting logs, if requested "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vis_fit_logs:\n",
    "    for log in fit_logs:\n",
    "        Fitter.plot_log(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca7024",
   "metadata": {},
   "source": [
    "## Create folder to save results into if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = pathlib.Path(save_loc)\n",
    "type_save_loc = save_folder / (rs_type + '_images')\n",
    "if not os.path.isdir(save_loc):\n",
    "    print('Creating folder to save plots into: ' + str(save_folder))\n",
    "    os.makedirs(save_folder)\n",
    "    \n",
    "if not os.path.isdir(type_save_loc):\n",
    "    print('Creating folder to save plots into: ' + str(type_save_loc))\n",
    "    os.makedirs(type_save_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1782d54",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper formatting function\n",
    "def format_box(ax):\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.xlabel('property dim 1 (a.u.)')\n",
    "    plt.ylabel('property dim 2 (a.u.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10831b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for evaluating model performance\n",
    "\n",
    "def eval_mdl_perf(eval_x: torch.Tensor, eval_y: torch.Tensor, coll, priors, n_elbo_smps=100):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = predict(coll=coll, x=eval_x, sample=False).cpu().numpy()\n",
    "\n",
    "    # Measure performance of the predictions\n",
    "    eval_r_sq = r_squared(eval_y.cpu().numpy(), y_hat) \n",
    "\n",
    "    \n",
    "    eval_corr = corr(eval_y.cpu().numpy(), y_hat) \n",
    "    \n",
    "    # Measure the ELBO for the fit models on the data in the training domain\n",
    "    orig_data = coll.data\n",
    "    coll.data = [eval_x, eval_y]\n",
    "    with torch.no_grad():\n",
    "        eval_elbo = approximate_elbo(coll=coll, priors=priors, n_smps=n_elbo_smps, \n",
    "                                     skip_s_in_kl=True, skip_b_in_kl=True, skip_s_out_kl=True, \n",
    "                                     skip_b_out_kl=True)\n",
    "    \n",
    "    return {'r_sq': eval_r_sq, 'corr': eval_corr, 'elbo': eval_elbo['elbo'].cpu().numpy(), \n",
    "            'decomposed_elbo': eval_elbo, 'y_true': eval_y.cpu().numpy(), 'y_hat': y_hat}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vis_perf(eval_rs: dict, ax, metric='r_sq', min_vl=None, min_plot_vl = None, max_plot_vl = None, \n",
    "             title=None, ex_i = None):\n",
    "    \"\"\" \n",
    "    Plots relative performance of models fit to combined data vs individual data. \n",
    "    \n",
    "    In particular makes scatter plots of performance comparing what happens when we fit \n",
    "    models together with DPMS vs. fitting to individual example systems alone\n",
    "    \"\"\"\n",
    "    \n",
    "    comb_train_domain_vls = np.asarray([rs['comb_train_domain_perf'][metric].item() for rs in eval_rs])\n",
    "    sing_train_domain_vls = np.asarray([rs['sing_train_domain_perf'][metric].item() for rs in eval_rs])\n",
    "    \n",
    "    comb_full_vls = np.asarray([rs['comb_full_perf'][metric].item() for rs in eval_rs])\n",
    "    sing_full_vls = np.asarray([rs['sing_full_perf'][metric].item() for rs in eval_rs])\n",
    "    \n",
    "    # Enforce min value if we are suppose to\n",
    "    if min_vl is not None:\n",
    "        comb_train_domain_vls = bound(comb_train_domain_vls, min_vl, np.inf)\n",
    "        sing_train_domain_vls = bound(sing_train_domain_vls, min_vl, np.inf)\n",
    "        comb_full_vls = bound(comb_full_vls, min_vl, np.inf)\n",
    "        sing_full_vls = bound(sing_full_vls, min_vl, np.inf)\n",
    "\n",
    "    # Generate plot\n",
    "    all_vls = np.concatenate([comb_train_domain_vls, sing_train_domain_vls, comb_full_vls, sing_full_vls])\n",
    "    if min_plot_vl is None:\n",
    "        min_plot_vl = np.min(all_vls)\n",
    "    if max_plot_vl is None:\n",
    "        max_plot_vl= np.max(all_vls)\n",
    "    \n",
    "    ax.plot([min_plot_vl, max_plot_vl], [min_plot_vl, max_plot_vl], 'k--', label='_nolegend_')\n",
    "    ax.plot(sing_train_domain_vls, comb_train_domain_vls, 'k.',)\n",
    "    ax.plot(sing_full_vls, comb_full_vls, '.', color='gray')\n",
    "    \n",
    "    if ex_i is not None:\n",
    "        ax.plot(sing_train_domain_vls[ex_i], comb_train_domain_vls[ex_i], 'ko', markerfacecolor='none',\n",
    "                markersize=15)\n",
    "        ax.plot(sing_full_vls[ex_i], comb_full_vls[ex_i], 'o', markerfacecolor='none',\n",
    "                markersize=15, color='gray')\n",
    "    \n",
    "    plt.legend(['within training Distribution', 'out of training distribution'])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fit_to_true_cpd_scale(rs_type, fit_priors, true_priors, ang_range=None):\n",
    "    \"\"\"\n",
    "    Learns the best scale to match the fit CPD to the true CPD. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if rs_type == 'combined':\n",
    "        sc_grid_pts = pts\n",
    "    elif rs_type == 'single':   \n",
    "        sc_grid_pts, _ = list_grid_pts(grid_limits=np.asarray([[0, 1.0], [0, 1.0]]), n_pts_per_dim=[100,100])\n",
    "        sc_grid_pts = torch.tensor(sc_grid_pts[pts_in_arc(sc_grid_pts, np.asarray([.5, .5]), ang_range),:])\n",
    "\n",
    "    return  np.linalg.lstsq(fit_priors.w_prior(sc_grid_pts).detach().numpy(),\n",
    "                            true_priors.w_prior(sc_grid_pts).detach().numpy(), rcond=None)[0].item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a6fe60",
   "metadata": {},
   "source": [
    "## Visualize the active neurons for one example system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vis_min = np.min(ex_true_neuron_weights)\n",
    "w_vis_max = np.max(ex_true_neuron_weights)\n",
    "true_w_clrs = cm.viridis((ex_true_neuron_weights - w_vis_min)/(w_vis_max - w_vis_min)).squeeze()\n",
    "true_w_clrs[~ex_active_neurons,3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65c995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "sub_smp_int = 10\n",
    "\n",
    "# Plot active neurons \n",
    "ax.scatter(ex_neuron_props[::sub_smp_int,0], ex_neuron_props[::sub_smp_int,1], \n",
    "           marker='.', color=true_w_clrs[0::sub_smp_int,:], s=40)\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "ax.set_aspect('equal')\n",
    "plt.savefig(save_folder / ('active_neurons_s_' + str(ex_s_i) + '.pdf'), format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ed929",
   "metadata": {},
   "source": [
    "## Visualize the true and fit shared function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09922cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_true = rs['m_true']\n",
    "\n",
    "l_plot_pts = np.linspace(min_l_range,max_l_range,1000)\n",
    "m_true_pts = m_true(torch.tensor(l_plot_pts)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15132076",
   "metadata": {},
   "source": [
    "### We apply a scale and offset to the domain of the fit function to best match the true function   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aa91f",
   "metadata": {},
   "source": [
    "Here we look for the best initialization scale and offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d0a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialization options we search over\n",
    "weight_vls = np.linspace(-10, 10, 10)\n",
    "bias_vls = np.linspace(-1, 1, 10)\n",
    "er_vls = np.zeros([len(weight_vls), len(bias_vls)])\n",
    "\n",
    "best_er = np.inf\n",
    "best_inds = (None, None)\n",
    "for w_i, w_v in enumerate(weight_vls):\n",
    "    for b_i, b_v in enumerate(bias_vls):\n",
    "        m_fit_cp = copy.deepcopy(m_fit)\n",
    "        for param in m_fit_cp.parameters():\n",
    "            param.requires_grad = False\n",
    "        m_fit_scaled = torch.nn.Sequential(torch.nn.Linear(1, 1, bias=True), m_fit_cp)\n",
    "        m_fit_scaled[0].weight.data[0] = w_v\n",
    "        m_fit_scaled[0].bias.data[0] = b_v\n",
    "\n",
    "        er = match_torch_module(m_true, m_fit_scaled, m_fit_range, \n",
    "                                optim_opts={'lr': .001}, n_its=500, update_int=6000)\n",
    "        \n",
    "        er = er.detach().numpy()\n",
    "        if er < best_er:\n",
    "            best_er = er\n",
    "            best_inds = (w_i, b_i)\n",
    "            print(best_er)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fit_scaled = torch.nn.Sequential(torch.nn.Linear(1, 1, bias=True), m_fit_cp)\n",
    "m_fit_scaled[0].weight.data[0] = weight_vls[best_inds[0]]\n",
    "m_fit_scaled[0].bias.data[0] = bias_vls[best_inds[0]]\n",
    "match_torch_module(m_true, m_fit_scaled, m_fit_range, \n",
    "                   optim_opts={'lr': .001}, n_its=50000, update_int=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0fde8",
   "metadata": {},
   "source": [
    "Plot the scaled and offset fit shared function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c033db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_int = np.asarray(rs['ind_intervals'][ex_s_i])/np.sqrt(rs['ps']['n_input_var_range'][0])\n",
    "ex_int_w = ex_int[1] - ex_int[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd99275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fit_pts = m_fit_scaled(torch.tensor(l_plot_pts)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60103eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "ax.add_patch(Rectangle([ex_int[0], -1.5], ex_int_w, 3.0))\n",
    "ax.plot(l_plot_pts, m_true_pts, 'k-', linewidth=4)\n",
    "ax.plot(l_plot_pts, m_fit_pts, 'r-')\n",
    "plt.savefig(type_save_loc / 'm_fit.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac7c5e",
   "metadata": {},
   "source": [
    "## Visualize the fit mean of the posterior over weights for the example subject "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57767ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_to_w_sc = np.linalg.lstsq(ex_w_fit_mn[ex_active_neurons], \n",
    "                               ex_true_neuron_weights[ex_active_neurons], rcond=None)[0].item()\n",
    "w_fit_mn_scaled = true_to_w_sc*ex_w_fit_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_w_clrs = cm.viridis((w_fit_mn_scaled - w_vis_min)/(w_vis_max - w_vis_min)).squeeze()\n",
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "# Plot active neurons \n",
    "ax.scatter(ex_neuron_props[::sub_smp_int,0], ex_neuron_props[::sub_smp_int,1], \n",
    "           marker='.', color=fit_w_clrs[::sub_smp_int,:], s=40)\n",
    "\n",
    "format_box(ax)\n",
    "plt.savefig(type_save_loc / ('posterior_weights_s_' + str(ex_s_i) + '.pdf'), format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d470f0",
   "metadata": {},
   "source": [
    "## Visualize the mean and standard deviation of the true CPD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77686b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts, dim_pts = list_grid_pts(grid_limits=np.asarray([[0, 1.0], [0, 1.0]]), n_pts_per_dim=[100,100])\n",
    "pts = torch.tensor(pts, dtype=torch.float)\n",
    "\n",
    "true_w_mn = true_priors.w_prior(pts).detach().numpy()\n",
    "true_w_std = np.concatenate([d.std_f(pts).detach().cpu().numpy() for d in true_priors.w_prior.dists], axis=1)\n",
    "true_w_mn_im = true_w_mn.reshape([100,100]).transpose()\n",
    "true_w_std_im = true_w_std.reshape([100,100]).transpose()\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(1,1,1)\n",
    "im = ax.imshow(true_w_mn_im, origin='lower', extent=[0, 1.0, 0, 1.0], vmin=w_vis_min, vmax=w_vis_max)\n",
    "plt.colorbar(im)\n",
    "format_box(ax)\n",
    "plt.title('True CPD Mean')\n",
    "plt.savefig(save_folder / 'true_cpd_mean.pdf', format='pdf')\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(1,1,1)\n",
    "im = ax.imshow(true_w_std_im, origin='lower', extent=[0, 1.0, 0, 1.0])\n",
    "std_vmin, std_vmax = im.get_clim() # Keep track of color limits\n",
    "plt.colorbar(im)\n",
    "format_box(ax)\n",
    "plt.title('True CPD Standard Deviation')\n",
    "plt.savefig(save_folder / 'true_cpd_std.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7814de4",
   "metadata": {},
   "source": [
    "## Visualize the mean and standard deviation of the fit CPD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5f2fc",
   "metadata": {},
   "source": [
    "Sample fit CPD in a grid for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49acff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_w_mn = fit_priors.w_prior(pts).detach().numpy()\n",
    "fit_w_std = np.concatenate([d.std_f(pts).detach().cpu().numpy() for d in fit_priors.w_prior.dists], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1516cfb",
   "metadata": {},
   "source": [
    "Learn the best scale to match the fit CPD to the true CPD - we use the portion of space only where it should be possible to learn the CPD for this.  This should produce the best correspondance between the true and fit CPD for models when we apply DPMS to subjects in isolation.  Because we want to show that using DPMS with many subjects is better than fitting to subjects individually, this produces a conservative visualization for the purposes of our comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3988956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle range is only used for single fits\n",
    "true_to_fit_cpd_sc = get_fit_to_true_cpd_scale(rs_type, fit_priors, true_priors, ang_range=rs['ang_ranges'][ex_s_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_w_mn_scaled = fit_w_mn*true_to_fit_cpd_sc\n",
    "fit_w_std_scaled = fit_w_std*np.abs(true_to_fit_cpd_sc)\n",
    "\n",
    "fit_w_mn_scaled_im = fit_w_mn_scaled.reshape([100,100]).transpose()\n",
    "fit_w_std_scaled_im = fit_w_std_scaled.reshape([100,100]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(1,1,1)\n",
    "im = ax.imshow(fit_w_mn_scaled_im, origin='lower', extent=[0, 1.0, 0, 1.0], vmin=w_vis_min, vmax=w_vis_max)\n",
    "plt.colorbar(im)\n",
    "format_box(ax)\n",
    "plt.title('Fit CPD Mean')\n",
    "plt.savefig(type_save_loc / 'fit_cpd_mean.pdf', format='pdf')\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(1,1,1)\n",
    "im = ax.imshow(fit_w_std_scaled_im, origin='lower', extent=[0, 1.0, 0, 1.0])#, vmin=std_vmin, vmax=std_vmax)\n",
    "plt.colorbar(im)\n",
    "format_box(ax)\n",
    "plt.title('Fit CPD Standard Deviation')\n",
    "plt.savefig(type_save_loc / 'fit_cpd_std.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc135b3",
   "metadata": {},
   "source": [
    "## Make a scatter plot of true vs fit mean for example subject \n",
    "\n",
    "Before plotting we first scale the fit weights to account for the non-identifiability in scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3334ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "pts = np.concatenate([ex_true_neuron_weights, true_to_fit_cpd_sc*ex_w_fit_mn], axis=1)\n",
    "clrs = np.zeros([pts.shape[0], 3])\n",
    "clrs[~ex_active_neurons,:] = .8\n",
    "    \n",
    "# Here we permute the plotting order of the points so we don't create\n",
    "# visualization artefacts of regions that look like they are all black or red.\n",
    "rand_order = np.random.permutation(pts.shape[0])\n",
    "\n",
    "plt.xlabel('True W')\n",
    "plt.ylabel('W Post Mn')\n",
    "ax.scatter(pts[rand_order,0], pts[rand_order,1], marker='.', color=clrs[rand_order,:], s=1)\n",
    "ax.set_aspect('equal')\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "# Save the figure - use jpg due to large number of vector objects a eps/pdf would create\n",
    "plt.savefig(type_save_loc / ('true_vs_est_w_s_' + str(ex_s_i) + '.jpg'), format='jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1331c",
   "metadata": {},
   "source": [
    "## Evaluate model performance on test data for models fit to each example system together and individually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ff000",
   "metadata": {},
   "source": [
    "Evaluate model performance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b73192",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rs = [None]*n_ex_systems\n",
    "for i, s_i in enumerate(single_fit_inds):\n",
    "    \n",
    "    # ============================================================================================================\n",
    "    # Generate data using the ground truth example system in two ways:\n",
    "    #    1) The first is using the same active neurons and behavior range that was observed in the training data\n",
    "    #    2 The second is using all neurons and all behavioral ranges\n",
    "    \n",
    "    # Here we generate data with the same active neurons and behevioral range as was in the training data for\n",
    "    # this subject\n",
    "    with torch.no_grad():\n",
    "        x_i_train_domain = efficient_cone_and_projected_interval_sample(n_smps=n_eval_smps,\n",
    "                                                                        locs=rs['ind_props'][s_i],\n",
    "                                                                        ctr=torch.tensor([.5, .5]),\n",
    "                                                                        ang_range=rs['ang_ranges'][s_i],\n",
    "                                                                        w=rs['ind_true_mdls'][s_i].w.detach(),\n",
    "                                                                        interval=rs['ind_intervals'][s_i],\n",
    "                                                                        big_std=1.0,\n",
    "                                                                        small_std=0,\n",
    "                                                                        device=compute_device)\n",
    "    \n",
    "        # Before generating y data, move model to GPU (if possible)\n",
    "        rs['ind_true_mdls'][s_i].to(compute_device)\n",
    "        y_i_train_domain = rs['ind_true_mdls'][s_i].forward(x=x_i_train_domain)\n",
    "    \n",
    "        # Here we generate data when all neurons are active and for the full behavioral range\n",
    "        x_i_full = efficient_cone_and_projected_interval_sample(n_smps=n_eval_smps,\n",
    "                                                                locs=rs['ind_props'][s_i],\n",
    "                                                                ctr=torch.tensor([.5, .5]),\n",
    "                                                                ang_range=[0, 2*np.pi],\n",
    "                                                                w=rs['ind_true_mdls'][s_i].w.detach(),\n",
    "                                                                interval=[min_interval, max_interval],\n",
    "                                                                big_std=1.0,\n",
    "                                                                small_std=0,\n",
    "                                                                device=compute_device)\n",
    "        \n",
    "        y_i_full = rs['ind_true_mdls'][s_i].forward(x=x_i_full)\n",
    "        \n",
    "        # Move true model back to cpu\n",
    "        rs['ind_true_mdls'][s_i].to('cpu')\n",
    "        \n",
    "    # ============================================================================================================\n",
    "    # Now we evaluate performance of the models fit collectively and individually \n",
    "    sing_i = np.argwhere(np.asarray(single_fit_inds) == s_i).item()\n",
    "    \n",
    "    comb_coll = comb_vi_colls[s_i]\n",
    "    comb_coll.props = rs['ind_props'][s_i]\n",
    "    comb_priors = comb_priors\n",
    "    \n",
    "    sing_coll = single_fits[sing_i]['vi_coll']\n",
    "    sing_coll.props = rs['ind_props'][s_i]\n",
    "    sing_priors = single_fits[sing_i]['priors']\n",
    "\n",
    "    # Move the collections and priors to GPU (if possible) \n",
    "    comb_coll.to(compute_device)\n",
    "    sing_coll.to(compute_device)\n",
    "    comb_priors.to(compute_device)\n",
    "    sing_priors.to(compute_device)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    comb_train_domain_perf = eval_mdl_perf(x_i_train_domain, y_i_train_domain, comb_coll, comb_priors)\n",
    "    sing_train_domain_perf = eval_mdl_perf(x_i_train_domain, y_i_train_domain, sing_coll, sing_priors)\n",
    "    \n",
    "    comb_full_perf = eval_mdl_perf(x_i_full, y_i_full, comb_coll, comb_priors)\n",
    "    sing_full_perf = eval_mdl_perf(x_i_full, y_i_full, sing_coll, sing_priors)\n",
    "    \n",
    "    eval_rs[i] = {'s_i': s_i, \n",
    "                  'comb_train_domain_perf': comb_train_domain_perf,\n",
    "                  'sing_train_domain_perf': sing_train_domain_perf,\n",
    "                  'comb_full_perf': comb_full_perf, \n",
    "                  'sing_full_perf': sing_full_perf}\n",
    "    \n",
    "    # Move everything back to cpu\n",
    "    comb_coll.to('cpu')\n",
    "    sing_coll.to('cpu')\n",
    "    comb_priors.to('cpu')\n",
    "    sing_priors.to('cpu')\n",
    "    \n",
    "                  \n",
    "    print('Done evaluating model performance for subject ' + str(s_i) + '.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bc5ed0",
   "metadata": {},
   "source": [
    "## Visualize model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18763841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "vis_perf(eval_rs, ax, 'r_sq', min_vl=-1, max_plot_vl=1, title='R-Squared', ex_i=ex_s_i)\n",
    "plt.savefig(save_folder / 'perf_r_sq.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ef12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "vis_perf(eval_rs, ax, 'corr', max_plot_vl=1, title='Correlation', ex_i=ex_s_i)\n",
    "plt.savefig(save_folder / 'perf_corr.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c893b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "vis_perf(eval_rs, ax, 'elbo', min_vl=-600000.0, title='ELBO', ex_i=ex_s_i)\n",
    "plt.savefig(save_folder / 'perf_elbo.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea0ebf6",
   "metadata": {},
   "source": [
    "## Debug code goes here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_corr = [eval_rs[i]['sing_train_domain_perf']['corr'] for i in range(n_ex_systems)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0989d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(sing_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626c3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.asarray(sing_corr) > .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89766ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e68334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c65cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ca72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
